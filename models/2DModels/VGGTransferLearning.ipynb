{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLOACV0wXrznhvCw5glkDQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoFesta/3DFER_SE4AI/blob/main/models/2DModels/VGGTransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f6qDfHlykkz3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from torchvision.datasets import ImageFolder\n",
        "TRAIN_PATH = \"train\"\n",
        "TEST_PATH =\"test\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uLobkPskqmQ",
        "outputId": "9c7c695e-9e3f-4859-e323-ef2d9f2f078f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mlflow\n",
        "!databricks configure --host https://community.cloud.databricks.com/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1h0EO8akzq7",
        "outputId": "a8f31c9e-f157-445c-87f8-e53ad77b5e93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Username: gfesta24@gmail.com\n",
            "Password: \n",
            "Repeat for confirmation: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/Shareddrives/Datasets SEFAI/fer2013.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "5Ldu44NukuL9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_std_calc(loader):\n",
        "  cnt = 0\n",
        "  fst_moment = torch.empty(3)\n",
        "  snd_moment = torch.empty(3)\n",
        "\n",
        "  for images, _ in loader:\n",
        "      b, c, h, w = images.shape\n",
        "      nb_pixels = b * h * w\n",
        "      sum_ = torch.sum(images, dim=[0, 2, 3])\n",
        "      sum_of_square = torch.sum(images ** 2,\n",
        "                                dim=[0, 2, 3])\n",
        "      fst_moment = (cnt * fst_moment + sum_) / (\n",
        "                    cnt + nb_pixels)\n",
        "      snd_moment = (cnt * snd_moment + sum_of_square) / (\n",
        "                          cnt + nb_pixels)\n",
        "      cnt += nb_pixels\n",
        "\n",
        "  mean, std = fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)\n",
        "  return mean, std"
      ],
      "metadata": {
        "id": "FPTAb1_wk7Ca"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"train\" # Directory containing the training data\n",
        "test_dir = \"test\"  # Directory containing the validation data\n",
        "#Normalizzazione train loader\n",
        "\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=transforms.ToTensor())\n",
        "test_dataset = ImageFolder(test_dir, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader =  DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "mean, std = mean_std_calc(train_loader)\n",
        "\n",
        "# Define the transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "\n",
        "])\n",
        "\n",
        "test_loader =  DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "mean, std = mean_std_calc(test_loader)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "test_dataset = ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "PSLuXCrnlL4j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  probabilities = torch.nn.functional.softmax(preds, dim=1)\n",
        "  _, predicted = torch.max(probabilities, dim=1)\n",
        "  n_correct = (predicted==labels).sum().float()\n",
        "\n",
        "  acc =n_correct / labels.shape[0]\n",
        "  acc= torch.round(acc*100)\n",
        "  return acc, n_correct;"
      ],
      "metadata": {
        "id": "VZquwQQNlPbr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "        self.relu8 = nn.ReLU(inplace=True)\n",
        "        self.dropout8 = nn.Dropout(p=0.5)\n",
        "        self.fc9 = nn.Linear(in_features=2622, out_features=7, bias=True)\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        x38 = self.fc8(x37)\n",
        "        x39 = self.relu8(x38)\n",
        "        x40 = self.dropout8(x39)\n",
        "        x41 = self.fc9(x40)\n",
        "        return x41\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    pretrained_dict = torch.load(weights_path)\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # I pesi caricati vengono filtrati per assicurarsi che vengano caricati solo quelli di layer/neuroni effettivamente presenti\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "    # Carica i pesi preaddestrati nel modello personalizzato\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Freeze dei pesi già presenti (tranne ultimo layer)\n",
        "    #for name, param in model.named_parameters():\n",
        "     #   if name not in ['fc8.weight', 'fc8.bias', 'relu8.weight', 'relu8.bias', 'dropout8.weight', 'dropout8.bias', 'fc9.weight', 'fc9.bias']:\n",
        "      #      param.requires_grad = False\n",
        "      #Commentato perché funziona peggio rispetto a che tutti i pesi subiscano il grad. desc.\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "X5drZiKulUgw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = vgg_face_dag(weights_path='/content/drive/Shareddrives/Datasets SEFAI/vgg-pretrained/robots.ox.ac.uk_~albanie_models_pytorch-mcn_vgg_face_dag.pth').to(device)\n"
      ],
      "metadata": {
        "id": "uWNPn755lXo5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Users/gfesta24@gmail.com/VGGTransferLearning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O6IQrBmlabZ",
        "outputId": "dd3a9fc6-3000-452b-b4b3-821752c671dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/1645340172644811', creation_time=1687363891584, experiment_id='1645340172644811', last_update_time=1687364666642, lifecycle_stage='active', name='/Users/gfesta24@gmail.com/VGGTransferLearning', tags={'mlflow.experiment.sourceName': '/Users/gfesta24@gmail.com/VGGTransferLearning',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'gfesta24@gmail.com',\n",
              " 'mlflow.ownerId': '1923923806180228'}>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.start_run()\n",
        "\n",
        "mlflow.set_tag(\"model_name\", \"VGGTransferLearning\")\n",
        "mlflow.log_param(\"lr\", 0.001)\n",
        "mlflow.log_param(\"batch_size\", 64)\n",
        "\n",
        "acc_list_train=[]\n",
        "acc_list_test=[]\n",
        "\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum =0.9)\n",
        "\n",
        "patience = 3\n",
        "\n",
        "\n",
        "\n",
        "best_loss = 100\n",
        "counter=0\n",
        "stop=False\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        print(counter)\n",
        "        if stop:\n",
        "          print(stop)\n",
        "          break\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0\n",
        "        seen = 0\n",
        "        for images, labels in train_loader:\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "\n",
        "          outputs = model(images)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          _, acc = accuracy(outputs, labels)\n",
        "          seen +=labels.shape[0]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "          running_acc += acc\n",
        "\n",
        "        print (f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Acc: {running_acc/seen:.4f}')\n",
        "        acc_list_train.append(running_acc/len(train_loader))\n",
        "        mlflow.log_metric(\"train_loss\", running_loss / len(train_loader), step=epoch)\n",
        "        mlflow.log_metric(\"train_acc\", running_acc/seen, step=epoch)\n",
        "        model.eval()\n",
        "\n",
        "        tot_corrette = 0\n",
        "        tot_eseguite = 0\n",
        "        running_test_loss = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          for images, labels in test_loader:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              outputs = model(images)\n",
        "              test_loss = criterion(outputs, labels)\n",
        "              _, n_corrette=accuracy(outputs, labels)\n",
        "\n",
        "              running_test_loss += test_loss.item()\n",
        "              tot_corrette+=n_corrette.item()\n",
        "              tot_eseguite+=labels.shape[0]\n",
        "\n",
        "          test_acc=100* (tot_corrette/tot_eseguite)\n",
        "          val_loss = running_test_loss / len(test_loader)\n",
        "          acc_list_test.append(test_acc)\n",
        "          print(\"Test acc: \", test_acc)\n",
        "          print(\"Test loss: \", val_loss)\n",
        "          mlflow.log_metric(\"test_acc\", test_acc, step=epoch)\n",
        "          mlflow.log_metric(\"test_loss\", val_loss, step=epoch)\n",
        "\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "          print(\"MIGLIORATO\")\n",
        "          torch.save(model.state_dict(), 'model_weights.pth')\n",
        "          best_loss = val_loss\n",
        "          best_model_train_acc=running_acc/seen\n",
        "          best_model_test_acc=test_acc\n",
        "          best_model_test_loss=val_loss\n",
        "          best_model_train_loss=running_loss / len(train_loader)\n",
        "          counter = 0\n",
        "          # Salva i pesi del modello se la validation loss è migliorata\n",
        "          torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "          counter += 1\n",
        "        # Verifica se raggiunto il criterio di early stopping\n",
        "          if counter >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              mlflow.log_artifact(\"best_model.pt\")\n",
        "              mlflow.log_metric(\"best_test_acc\", best_model_test_acc)\n",
        "              mlflow.log_metric(\"best_test_loss\", best_model_test_loss)\n",
        "              mlflow.log_metric(\"best_train_acc\", best_model_train_acc)\n",
        "              mlflow.log_metric(\"best_train_loss\", best_model_train_loss)\n",
        "              mlflow.end_run()\n",
        "              stop=True\n",
        "        print(\"BEST TEST LOSS: \", best_loss)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI6RItiNlgp4",
        "outputId": "ebf25a7b-5575-45a4-89ba-e738a41d938a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], Loss: 1.2556, Acc: 0.5312\n",
            "Test acc:  58.53998328225133\n",
            "Test loss:  1.1086255249987662\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1086255249987662\n",
            "0\n",
            "Epoch [1/100], Loss: 0.9627, Acc: 0.6422\n",
            "Test acc:  64.0289774310393\n",
            "Test loss:  0.9556759247737648\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  0.9556759247737648\n",
            "0\n",
            "Epoch [2/100], Loss: 0.8376, Acc: 0.6891\n",
            "Test acc:  65.0459738088604\n",
            "Test loss:  0.9479776574710829\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  0.9479776574710829\n",
            "0\n",
            "Epoch [3/100], Loss: 0.7202, Acc: 0.7368\n",
            "Test acc:  67.23321259403734\n",
            "Test loss:  0.9386598086726349\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  0.9386598086726349\n",
            "0\n",
            "Epoch [4/100], Loss: 0.6159, Acc: 0.7762\n",
            "Test acc:  64.43298969072166\n",
            "Test loss:  1.062602819462793\n",
            "BEST TEST LOSS:  0.9386598086726349\n",
            "1\n",
            "Epoch [5/100], Loss: 0.5223, Acc: 0.8124\n",
            "Test acc:  67.4282529952633\n",
            "Test loss:  0.9868801682528141\n",
            "BEST TEST LOSS:  0.9386598086726349\n",
            "2\n",
            "Epoch [6/100], Loss: 0.4417, Acc: 0.8410\n",
            "Test acc:  67.49791028141544\n",
            "Test loss:  1.0111388254218396\n",
            "Early stopping at epoch 7\n",
            "BEST TEST LOSS:  0.9386598086726349\n",
            "3\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/Shareddrives/Datasets SEFAI/scraped_pictures.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "i58kH2RHl9XJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "model = vgg_face_dag(weights_path='model_weights.pth').to(device)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Definisci le trasformazioni per il test dataset e per scraped_pictures\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Calcola le metriche sul test dataset\n",
        "model.eval()  # Imposta il modello in modalità di valutazione (non addestramento)\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(torch.nn.functional.softmax(outputs, dim=1), 1)\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "precision = precision_score(test_labels, test_predictions, average=None)\n",
        "f1 = f1_score(test_labels, test_predictions, average=None)\n",
        "#auc_roc = roc_auc_score(test_labels, test_predictions, multi_class='ovr')\n",
        "classification_rep = classification_report(test_labels, test_predictions)\n",
        "\n",
        "print(\"Test Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1 Score:\", f1)\n",
        "#print(\"AUC-ROC:\", auc_roc)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dptgA7dJmAWA",
        "outputId": "c94625ab-6702-44da-bda8-cdd5b19511b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "Accuracy: 0.6723321259403734\n",
            "Precision: [0.55828221 0.734375   0.58040936 0.86483455 0.65418118 0.52156863\n",
            " 0.78985507]\n",
            "F1 Score: [0.60695569 0.53714286 0.46487119 0.86702277 0.63082738 0.57472092\n",
            " 0.78842676]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.66      0.61       958\n",
            "           1       0.73      0.42      0.54       111\n",
            "           2       0.58      0.39      0.46      1024\n",
            "           3       0.86      0.87      0.87      1774\n",
            "           4       0.65      0.61      0.63      1233\n",
            "           5       0.52      0.64      0.57      1247\n",
            "           6       0.79      0.79      0.79       831\n",
            "\n",
            "    accuracy                           0.67      7178\n",
            "   macro avg       0.67      0.63      0.64      7178\n",
            "weighted avg       0.68      0.67      0.67      7178\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_dataset = ImageFolder(\"photos\", transform=test_transform)\n",
        "scraped_loader = DataLoader(scraped_dataset, batch_size=64, shuffle=False)\n",
        "# Calcola le metriche su scraped_pictures\n",
        "scraped_predictions = []\n",
        "scraped_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in scraped_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(torch.nn.functional.softmax(outputs, dim=1), 1)\n",
        "        scraped_predictions.extend(predictions.cpu().numpy())\n",
        "        scraped_labels.extend(labels)\n",
        "\n",
        "accuracy_scraped = accuracy_score(scraped_labels, scraped_predictions)\n",
        "precision_scraped = precision_score(scraped_labels, scraped_predictions, average=None)\n",
        "f1_scraped = f1_score(scraped_labels, scraped_predictions, average=None)\n",
        "#auc_roc_scraped = roc_auc_score(scraped_labels, scraped_predictions, multi_class='ovr')\n",
        "classification_rep_scraped = classification_report(scraped_labels, scraped_predictions)\n",
        "\n",
        "print(\"\\nMetrics for scraped_pictures:\")\n",
        "print(\"Accuracy:\", accuracy_scraped)\n",
        "print(\"Precision:\", precision_scraped)\n",
        "print(\"F1 Score:\", f1_scraped)\n",
        "#print(\"AUC-ROC:\", auc_roc_scraped)\n",
        "print(\"Classification Report:\\n\", classification_rep_scraped)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xxiG5NhmFgv",
        "outputId": "fd6e31c2-309f-48da-e403-2bc8355e27e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics for scraped_pictures:\n",
            "Accuracy: 0.29023746701846964\n",
            "Precision: [0.66666667 0.         0.17391304 0.33898305 0.15384615 0.16666667\n",
            " 0.43478261]\n",
            "F1 Score: [0.48275862 0.         0.15384615 0.40816327 0.21052632 0.24\n",
            " 0.26666667]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.38      0.48        74\n",
            "           1       0.00      0.00      0.00        74\n",
            "           2       0.17      0.14      0.15        29\n",
            "           3       0.34      0.51      0.41        78\n",
            "           4       0.15      0.33      0.21        30\n",
            "           5       0.17      0.43      0.24        42\n",
            "           6       0.43      0.19      0.27        52\n",
            "\n",
            "    accuracy                           0.29       379\n",
            "   macro avg       0.28      0.28      0.25       379\n",
            "weighted avg       0.30      0.29      0.27       379\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}