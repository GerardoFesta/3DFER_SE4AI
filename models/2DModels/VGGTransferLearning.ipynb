{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guXlKtPfDFeZ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from torchvision.datasets import ImageFolder\n",
        "TRAIN_PATH = \"train\"\n",
        "TEST_PATH =\"test\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpVebWmiJYqu",
        "outputId": "78f659a7-d152-4333-cbef-34f912425d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/Shareddrives/Datasets SEFAI/fer2013.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "VNe1wU3bMhWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mean_std_calc(loader):\n",
        "  cnt = 0\n",
        "  fst_moment = torch.empty(3)\n",
        "  snd_moment = torch.empty(3)\n",
        "\n",
        "  for images, _ in loader:\n",
        "      b, c, h, w = images.shape\n",
        "      nb_pixels = b * h * w\n",
        "      sum_ = torch.sum(images, dim=[0, 2, 3])\n",
        "      sum_of_square = torch.sum(images ** 2,\n",
        "                                dim=[0, 2, 3])\n",
        "      fst_moment = (cnt * fst_moment + sum_) / (\n",
        "                    cnt + nb_pixels)\n",
        "      snd_moment = (cnt * snd_moment + sum_of_square) / (\n",
        "                          cnt + nb_pixels)\n",
        "      cnt += nb_pixels\n",
        "\n",
        "  mean, std = fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)\n",
        "  return mean, std\n"
      ],
      "metadata": {
        "id": "nrZqSYYPQivB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"train\" # Directory containing the training data\n",
        "test_dir = \"test\"  # Directory containing the validation data\n",
        "#Normalizzazione train loader\n",
        "\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=transforms.ToTensor())\n",
        "test_dataset = ImageFolder(test_dir, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader =  DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "mean, std = mean_std_calc(train_loader)\n",
        "\n",
        "# Define the transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std),\n",
        "\n",
        "])\n",
        "\n",
        "test_loader =  DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "mean, std = mean_std_calc(test_loader)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "test_dataset = ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "rsnBWTvuPoRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  probabilities = torch.nn.functional.softmax(preds, dim=1)\n",
        "  _, predicted = torch.max(probabilities, dim=1)\n",
        "  n_correct = (predicted==labels).sum().float()\n",
        "\n",
        "  acc =n_correct / labels.shape[0]\n",
        "  acc= torch.round(acc*100)\n",
        "  return acc, n_correct;"
      ],
      "metadata": {
        "id": "_80BPtG_Q-WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Vgg_face_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_face_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU(inplace=True)\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU(inplace=True)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU(inplace=True)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU(inplace=True)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU(inplace=True)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU(inplace=True)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU(inplace=True)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU(inplace=True)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU(inplace=True)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU(inplace=True)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU(inplace=True)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU(inplace=True)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Linear(in_features=25088, out_features=4096, bias=True)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=2622, bias=True)\n",
        "        self.relu8 = nn.ReLU(inplace=True)\n",
        "        self.dropout8 = nn.Dropout(p=0.5)\n",
        "        self.fc9 = nn.Linear(in_features=2622, out_features=7, bias=True)\n",
        "\n",
        "    def forward(self, x0):\n",
        "        x1 = self.conv1_1(x0)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31_preflatten = self.pool5(x30)\n",
        "        x31 = x31_preflatten.view(x31_preflatten.size(0), -1)\n",
        "        x32 = self.fc6(x31)\n",
        "        x33 = self.relu6(x32)\n",
        "        x34 = self.dropout6(x33)\n",
        "        x35 = self.fc7(x34)\n",
        "        x36 = self.relu7(x35)\n",
        "        x37 = self.dropout7(x36)\n",
        "        x38 = self.fc8(x37)\n",
        "        x39 = self.relu8(x38)\n",
        "        x40 = self.dropout8(x39)\n",
        "        x41 = self.fc9(x40)\n",
        "        return x41\n",
        "\n",
        "def vgg_face_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_face_dag()\n",
        "    pretrained_dict = torch.load(weights_path)\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # I pesi caricati vengono filtrati per assicurarsi che vengano caricati solo quelli di layer/neuroni effettivamente presenti\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "    # Carica i pesi preaddestrati nel modello personalizzato\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "    # Freeze dei pesi già presenti (tranne ultimo layer)\n",
        "    #for name, param in model.named_parameters():\n",
        "     #   if name not in ['fc8.weight', 'fc8.bias', 'relu8.weight', 'relu8.bias', 'dropout8.weight', 'dropout8.bias', 'fc9.weight', 'fc9.bias']:\n",
        "      #      param.requires_grad = False\n",
        "      #Commentato perché funziona peggio rispetto a che tutti i pesi subiscano il grad. desc.\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "q2q1GAKgRGeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = vgg_face_dag(weights_path='/content/drive/Shareddrives/Datasets SEFAI/vgg-pretrained/robots.ox.ac.uk_~albanie_models_pytorch-mcn_vgg_face_dag.pth').to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "2zDVsmrUSscT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acc_list_train=[]\n",
        "acc_list_test=[]\n",
        "\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum =0.9)\n",
        "\n",
        "patience = 3\n",
        "\n",
        "\n",
        "\n",
        "best_loss = 100\n",
        "counter=0\n",
        "stop=False\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        print(counter)\n",
        "        if stop:\n",
        "          print(stop)\n",
        "          break\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0\n",
        "        seen = 0\n",
        "        for images, labels in train_loader:\n",
        "\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "\n",
        "          outputs = model(images)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          _, acc = accuracy(outputs, labels)\n",
        "          seen +=labels.shape[0]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "          running_acc += acc\n",
        "\n",
        "        print (f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Acc: {running_acc/seen:.4f}')\n",
        "        acc_list_train.append(running_acc/len(train_loader))\n",
        "        model.eval()\n",
        "\n",
        "        tot_corrette = 0\n",
        "        tot_eseguite = 0\n",
        "        running_test_loss = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          for images, labels in test_loader:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              outputs = model(images)\n",
        "              test_loss = criterion(outputs, labels)\n",
        "              _, n_corrette=accuracy(outputs, labels)\n",
        "\n",
        "              running_test_loss += test_loss.item()\n",
        "              tot_corrette+=n_corrette.item()\n",
        "              tot_eseguite+=labels.shape[0]\n",
        "\n",
        "          test_acc=100* (tot_corrette/tot_eseguite)\n",
        "          val_loss = running_test_loss / len(test_loader)\n",
        "          acc_list_test.append(test_acc)\n",
        "          print(\"Test acc: \", test_acc)\n",
        "          print(\"Test loss: \", val_loss)\n",
        "\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "          print(\"MIGLIORATO\")\n",
        "          torch.save(model.state_dict(), 'model_weights.pth')\n",
        "          best_loss = val_loss\n",
        "          best_model_train_acc=running_acc/seen\n",
        "          best_model_test_acc=test_acc\n",
        "          best_model_test_loss=val_loss\n",
        "          best_model_train_loss=running_loss / len(train_loader)\n",
        "          counter = 0\n",
        "          # Salva i pesi del modello se la validation loss è migliorata\n",
        "          torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "          counter += 1\n",
        "        # Verifica se raggiunto il criterio di early stopping\n",
        "          if counter >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              stop=True\n",
        "        print(\"BEST TEST LOSS: \", best_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "85hfPsiVU_M6",
        "outputId": "538f6e49-43d2-429a-c298-d4b207d88429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], Loss: 1.6768, Acc: 0.3486\n",
            "Test acc:  47.47840624129284\n",
            "Test loss:  1.4876196579595582\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.4876196579595582\n",
            "0\n",
            "Epoch [1/100], Loss: 1.3834, Acc: 0.4922\n",
            "Test acc:  52.57731958762887\n",
            "Test loss:  1.302372357750361\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.302372357750361\n",
            "0\n",
            "Epoch [2/100], Loss: 1.2931, Acc: 0.5165\n",
            "Test acc:  54.527723599888546\n",
            "Test loss:  1.247694233877469\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.247694233877469\n",
            "0\n",
            "Epoch [3/100], Loss: 1.2597, Acc: 0.5295\n",
            "Test acc:  55.48899414878796\n",
            "Test loss:  1.2169924779275878\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.2169924779275878\n",
            "0\n",
            "Epoch [4/100], Loss: 1.2372, Acc: 0.5391\n",
            "Test acc:  56.60351072722207\n",
            "Test loss:  1.1992898206267737\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1992898206267737\n",
            "0\n",
            "Epoch [5/100], Loss: 1.2151, Acc: 0.5482\n",
            "Test acc:  56.882139871830596\n",
            "Test loss:  1.1829513662156805\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1829513662156805\n",
            "0\n",
            "Epoch [6/100], Loss: 1.1975, Acc: 0.5540\n",
            "Test acc:  56.770688213987185\n",
            "Test loss:  1.1720840403463988\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1720840403463988\n",
            "0\n",
            "Epoch [7/100], Loss: 1.1909, Acc: 0.5576\n",
            "Test acc:  57.45332961827807\n",
            "Test loss:  1.1618311072872802\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1618311072872802\n",
            "0\n",
            "Epoch [8/100], Loss: 1.1728, Acc: 0.5642\n",
            "Test acc:  58.038450821955976\n",
            "Test loss:  1.1469860680862867\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1469860680862867\n",
            "0\n",
            "Epoch [9/100], Loss: 1.1660, Acc: 0.5637\n",
            "Test acc:  58.038450821955976\n",
            "Test loss:  1.1403497159481049\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1403497159481049\n",
            "0\n",
            "Epoch [10/100], Loss: 1.1524, Acc: 0.5710\n",
            "Test acc:  58.400668709947055\n",
            "Test loss:  1.1317688237249324\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1317688237249324\n",
            "0\n",
            "Epoch [11/100], Loss: 1.1416, Acc: 0.5743\n",
            "Test acc:  58.5817776539426\n",
            "Test loss:  1.1299095594249995\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1299095594249995\n",
            "0\n",
            "Epoch [12/100], Loss: 1.1409, Acc: 0.5776\n",
            "Test acc:  58.804680969629416\n",
            "Test loss:  1.120585284401885\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.120585284401885\n",
            "0\n",
            "Epoch [13/100], Loss: 1.1195, Acc: 0.5827\n",
            "Test acc:  58.44246308163834\n",
            "Test loss:  1.12067312002182\n",
            "BEST TEST LOSS:  1.120585284401885\n",
            "1\n",
            "Epoch [14/100], Loss: 1.1155, Acc: 0.5846\n",
            "Test acc:  59.23655614377263\n",
            "Test loss:  1.1125439144341291\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1125439144341291\n",
            "0\n",
            "Epoch [15/100], Loss: 1.1126, Acc: 0.5862\n",
            "Test acc:  59.612705488994145\n",
            "Test loss:  1.1062101563521192\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.1062101563521192\n",
            "0\n",
            "Epoch [16/100], Loss: 1.1020, Acc: 0.5908\n",
            "Test acc:  59.30621342992477\n",
            "Test loss:  1.100042605558328\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.100042605558328\n",
            "0\n",
            "Epoch [17/100], Loss: 1.0922, Acc: 0.5947\n",
            "Test acc:  60.253552521593754\n",
            "Test loss:  1.0934301997180533\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0934301997180533\n",
            "0\n",
            "Epoch [18/100], Loss: 1.0810, Acc: 0.5996\n",
            "Test acc:  60.26748397882419\n",
            "Test loss:  1.0905499302707942\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0905499302707942\n",
            "0\n",
            "Epoch [19/100], Loss: 1.0779, Acc: 0.6002\n",
            "Test acc:  60.39286709389802\n",
            "Test loss:  1.0852699464401312\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0852699464401312\n",
            "0\n",
            "Epoch [20/100], Loss: 1.0643, Acc: 0.6050\n",
            "Test acc:  60.65756478127612\n",
            "Test loss:  1.080672354033563\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.080672354033563\n",
            "0\n",
            "Epoch [21/100], Loss: 1.0531, Acc: 0.6106\n",
            "Test acc:  60.68542769573697\n",
            "Test loss:  1.0777039720421344\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0777039720421344\n",
            "0\n",
            "Epoch [22/100], Loss: 1.0484, Acc: 0.6135\n",
            "Test acc:  60.74115352465868\n",
            "Test loss:  1.0725618817109976\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0725618817109976\n",
            "0\n",
            "Epoch [23/100], Loss: 1.0383, Acc: 0.6131\n",
            "Test acc:  60.9083310114238\n",
            "Test loss:  1.0670014780707064\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0670014780707064\n",
            "0\n",
            "Epoch [24/100], Loss: 1.0329, Acc: 0.6167\n",
            "Test acc:  60.9083310114238\n",
            "Test loss:  1.0668746451888464\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0668746451888464\n",
            "0\n",
            "Epoch [25/100], Loss: 1.0270, Acc: 0.6222\n",
            "Test acc:  61.18696015603232\n",
            "Test loss:  1.0627202615801212\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0627202615801212\n",
            "0\n",
            "Epoch [26/100], Loss: 1.0188, Acc: 0.6280\n",
            "Test acc:  61.5491780440234\n",
            "Test loss:  1.0560373616429557\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0560373616429557\n",
            "0\n",
            "Epoch [27/100], Loss: 1.0105, Acc: 0.6284\n",
            "Test acc:  61.91139593201449\n",
            "Test loss:  1.054958065262938\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.054958065262938\n",
            "0\n",
            "Epoch [28/100], Loss: 0.9989, Acc: 0.6308\n",
            "Test acc:  61.36806910002787\n",
            "Test loss:  1.0499566627287231\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0499566627287231\n",
            "0\n",
            "Epoch [29/100], Loss: 0.9953, Acc: 0.6344\n",
            "Test acc:  61.78601281694065\n",
            "Test loss:  1.0530385364473394\n",
            "BEST TEST LOSS:  1.0499566627287231\n",
            "1\n",
            "Epoch [30/100], Loss: 0.9857, Acc: 0.6382\n",
            "Test acc:  61.93925884647534\n",
            "Test loss:  1.0426082748227414\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0426082748227414\n",
            "0\n",
            "Epoch [31/100], Loss: 0.9795, Acc: 0.6408\n",
            "Test acc:  61.82780718863194\n",
            "Test loss:  1.0438666396436438\n",
            "BEST TEST LOSS:  1.0426082748227414\n",
            "1\n",
            "Epoch [32/100], Loss: 0.9675, Acc: 0.6424\n",
            "Test acc:  62.14823070493174\n",
            "Test loss:  1.0412109887177965\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.0412109887177965\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c44a9964abb2>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m           \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/content/drive/Shareddrives/Datasets SEFAI/scraped_pictures.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "oWIzqCepBrPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, classification_report\n",
        "\n",
        "model = vgg_face_dag(weights_path='model_weights.pth').to(device)\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Definisci le trasformazioni per il test dataset e per scraped_pictures\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "# Calcola le metriche sul test dataset\n",
        "model.eval()  # Imposta il modello in modalità di valutazione (non addestramento)\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(torch.nn.functional.softmax(outputs, dim=1), 1)\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "precision = precision_score(test_labels, test_predictions, average=None)\n",
        "f1 = f1_score(test_labels, test_predictions, average=None)\n",
        "#auc_roc = roc_auc_score(test_labels, test_predictions, multi_class='ovr')\n",
        "classification_rep = classification_report(test_labels, test_predictions)\n",
        "\n",
        "print(\"Test Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"F1 Score:\", f1)\n",
        "#print(\"AUC-ROC:\", auc_roc)\n",
        "print(\"Classification Report:\\n\", classification_rep)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xm4YiBmf9sCv",
        "outputId": "7683c393-604e-406f-f903-c29f32098659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "Accuracy: 0.6613262747283366\n",
            "Precision: [0.55278311 0.70588235 0.52       0.85541506 0.59153005 0.55767563\n",
            " 0.76905312]\n",
            "F1 Score: [0.576      0.33103448 0.45614035 0.86612858 0.64219503 0.53583333\n",
            " 0.78491456]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.60      0.58       958\n",
            "           1       0.71      0.22      0.33       111\n",
            "           2       0.52      0.41      0.46      1024\n",
            "           3       0.86      0.88      0.87      1774\n",
            "           4       0.59      0.70      0.64      1233\n",
            "           5       0.56      0.52      0.54      1247\n",
            "           6       0.77      0.80      0.78       831\n",
            "\n",
            "    accuracy                           0.66      7178\n",
            "   macro avg       0.65      0.59      0.60      7178\n",
            "weighted avg       0.66      0.66      0.66      7178\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-fdc1156fdfa9>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscraped_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mscraped_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-4b1e8cb0c937>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x0)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx31_preflatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx31\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx31_preflatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx31_preflatten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mx32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mx33\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mx34\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x49 and 25088x4096)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scraped_dataset = ImageFolder(\"photos\", transform=test_transform)\n",
        "scraped_loader = DataLoader(scraped_dataset, batch_size=64, shuffle=False)\n",
        "# Calcola le metriche su scraped_pictures\n",
        "scraped_predictions = []\n",
        "scraped_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in scraped_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(torch.nn.functional.softmax(outputs, dim=1), 1)\n",
        "        scraped_predictions.extend(predictions.cpu().numpy())\n",
        "        scraped_labels.extend(labels)\n",
        "\n",
        "accuracy_scraped = accuracy_score(scraped_labels, scraped_predictions)\n",
        "precision_scraped = precision_score(scraped_labels, scraped_predictions, average=None)\n",
        "f1_scraped = f1_score(scraped_labels, scraped_predictions, average=None)\n",
        "#auc_roc_scraped = roc_auc_score(scraped_labels, scraped_predictions, multi_class='ovr')\n",
        "classification_rep_scraped = classification_report(scraped_labels, scraped_predictions)\n",
        "\n",
        "print(\"\\nMetrics for scraped_pictures:\")\n",
        "print(\"Accuracy:\", accuracy_scraped)\n",
        "print(\"Precision:\", precision_scraped)\n",
        "print(\"F1 Score:\", f1_scraped)\n",
        "#print(\"AUC-ROC:\", auc_roc_scraped)\n",
        "print(\"Classification Report:\\n\", classification_rep_scraped)\n"
      ],
      "metadata": {
        "id": "B2uDBCTIYmB0",
        "outputId": "0aaf1ba0-b299-4800-b607-08d3716db5e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics for scraped_pictures:\n",
            "Accuracy: 0.31926121372031663\n",
            "Precision: [0.46153846 0.         0.11111111 0.31506849 0.25       0.30188679\n",
            " 0.66666667]\n",
            "F1 Score: [0.47368421 0.         0.15217391 0.41071429 0.22222222 0.33684211\n",
            " 0.29850746]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.49      0.47        74\n",
            "           1       0.00      0.00      0.00        74\n",
            "           2       0.11      0.24      0.15        29\n",
            "           3       0.32      0.59      0.41        78\n",
            "           4       0.25      0.20      0.22        30\n",
            "           5       0.30      0.38      0.34        42\n",
            "           6       0.67      0.19      0.30        52\n",
            "\n",
            "    accuracy                           0.32       379\n",
            "   macro avg       0.30      0.30      0.27       379\n",
            "weighted avg       0.31      0.32      0.28       379\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}