{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoFesta/3DFER_SE4AI/blob/main/models/3DModels/VoxelCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wQY5oZcsfnf",
        "outputId": "f28dba1e-7899-4bd9-d678-5adbc1c37794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mlflow\n",
        "!databricks configure --host https://community.cloud.databricks.com/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOo2RTYRDoub",
        "outputId": "9d0f2f98-1473-42b7-9f11-beeefe2b7394"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Username: gfesta24@gmail.com\n",
            "Password: \n",
            "Repeat for confirmation: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ParameterGrid"
      ],
      "metadata": {
        "id": "kDQsV4Ersjxk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = np.load('/content/drive/Shareddrives/Datasets SEFAI/voxel_train_dataset.npz')\n",
        "test_df = np.load('/content/drive/Shareddrives/Datasets SEFAI/voxel_test_dataset.npz')"
      ],
      "metadata": {
        "id": "j2PFiOads0Tp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X_array, Y_array, transform=None):\n",
        "        self.X = X_array\n",
        "        self.Y = Y_array\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.X[index]\n",
        "        label = self.Y[index]\n",
        "\n",
        "        # Esegui le trasformazioni se definite\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "q3E2RQbjvyQ7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Users/gfesta24@gmail.com/Voxels3DCNN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT8dV3hDEqYI",
        "outputId": "b726c717-7fef-4780-9f70-3c884d902a2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/2211619870750348', creation_time=1686564463983, experiment_id='2211619870750348', last_update_time=1687355031830, lifecycle_stage='active', name='/Users/gfesta24@gmail.com/Voxels3DCNN', tags={'mlflow.experiment.sourceName': '/Users/gfesta24@gmail.com/Voxels3DCNN',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'gfesta24@gmail.com',\n",
              " 'mlflow.ownerId': '1923923806180228'}>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= train_df['data']\n",
        "X_test = test_df['data']\n",
        "y_train = train_df['labels']\n",
        "y_test = test_df['labels']\n",
        "\n",
        "X_full = np.concatenate((X_train, X_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_test), axis=0)\n"
      ],
      "metadata": {
        "id": "s5V8JQ_7tKOz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.33, random_state=42, stratify=y_full)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n",
        "\n",
        "test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n",
        "\n",
        "batch_size=64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Ch6KCfL9uFa2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  probabilities = torch.nn.functional.softmax(preds, dim=1)\n",
        "  _, predicted = torch.max(probabilities, dim=1)\n",
        "  n_correct = (predicted==labels).sum().float()\n",
        "\n",
        "  acc =n_correct / labels.shape[0]\n",
        "  acc= torch.round(acc*100)\n",
        "  return acc, n_correct;"
      ],
      "metadata": {
        "id": "xoBlSQbPxg14"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(1, 32, kernel_size=(5, 5, 3), stride=1, padding=(2, 2, 1))\n",
        "        self.batchnorm1 = nn.BatchNorm3d(32)\n",
        "        self.maxpool1 = nn.MaxPool3d(kernel_size=(3, 3, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 32, kernel_size=(3, 3, 3), stride=1, padding=0)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(32)\n",
        "        self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(2560, 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.maxpool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.maxpool2(x)\n",
        "\n",
        "\n",
        "        print(x.shape)\n",
        "        x = self.flatten(x)\n",
        "        print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGNaNRWvxdLR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(1, 32, kernel_size=(5, 5, 3), stride=1, padding=(2, 2, 1))\n",
        "        self.batchnorm1 = nn.BatchNorm3d(32)\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), stride=1, padding=0)\n",
        "        self.batchnorm2 = nn.BatchNorm3d(64)\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), stride=1, padding=0)\n",
        "        self.batchnorm3 = nn.BatchNorm3d(128)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3456, 512)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        #x = self.maxpool(x)\n",
        "\n",
        "        # Calcolo dinamico della dimensione dell'input per fc1\n",
        "        batch_size = x.size(0)\n",
        "        #print(x.shape)\n",
        "        x = self.flatten(x)  # Flatten\n",
        "        #print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6Nhg1FOrKPWa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "id": "2UsVipO7Oxw3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'batch_size': [64, 128, 256, 512],'lr': [0.01, 0.001, 0.0001], \"momentum\":[0.7, 0.8, 0.9], \"decay\":[0.001, 0.01, 0.1]}\n",
        "expanded_grid = ParameterGrid(param_grid)\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "experiment = client.get_experiment_by_name(\"/Users/gfesta24@gmail.com/Voxels3DCNN\")\n",
        "\n",
        "\n",
        "for i in range(len(expanded_grid)):\n",
        "\n",
        "  runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], filter_string=\" and \".join([f\"params.{k} = '{v}'\" for k, v in expanded_grid[i].items()] + [\"tags.model_name = 'VoxelCNN3layer'\"]))\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=expanded_grid[i]['batch_size'], shuffle=True)\n",
        "\n",
        "  test_loader = DataLoader(test_dataset, batch_size=expanded_grid[i]['batch_size'], shuffle=False)\n",
        "\n",
        "  if len(runs) == 0:\n",
        "\n",
        "\n",
        "    best_loss=100\n",
        "    best_model_train_acc=0\n",
        "    best_model_test_acc=0\n",
        "    best_model_test_loss=0\n",
        "    best_model_train_loss=0\n",
        "    min_delta=0\n",
        "    patience=3\n",
        "    model = MyModel(num_classes = 7)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=expanded_grid[i]['lr'], momentum =expanded_grid[i]['momentum'])\n",
        "\n",
        "\n",
        "    mlflow.start_run()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    acc_list_train=[]\n",
        "    acc_list_test=[]\n",
        "    model.train()\n",
        "\n",
        "    mlflow.set_tag(\"model_name\", \"VoxelCNN3layer\")\n",
        "    mlflow.log_param(\"lr\", expanded_grid[i]['lr'])\n",
        "    mlflow.log_param(\"momentum\", expanded_grid[i]['momentum'])\n",
        "    mlflow.log_param(\"batch_size\", expanded_grid[i]['batch_size'])\n",
        "    mlflow.log_param(\"decay\", expanded_grid[i]['decay'])\n",
        "    best_loss = 100\n",
        "    counter=0\n",
        "    stop=False\n",
        "    for epoch in range(num_epochs):\n",
        "        if stop:\n",
        "          break\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        tot_seen = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = torch.reshape(X_batch, (X_batch.shape[0], 1, 24, 24, 24))\n",
        "            X_batch = X_batch.type(torch.cuda.FloatTensor)\n",
        "            #manda i batch al device\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            #azzera gradiente\n",
        "            optimizer.zero_grad()\n",
        "            #predict\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "\n",
        "            #print(y_batch.shape)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            _,acc = accuracy(y_pred, y_batch)\n",
        "            #backpropagation della loss\n",
        "            loss.backward()\n",
        "            #ottimizzazione\n",
        "            optimizer.step()\n",
        "\n",
        "            #somma della loss e dell'accuracy per il batch\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc\n",
        "            tot_seen += len(y_batch)\n",
        "\n",
        "\n",
        "        print(f'Epoch {epoch}: | Loss: {running_loss/len(train_loader):.5f} | Acc: {running_acc/tot_seen:.3f}')\n",
        "        acc_list_train.append(running_acc/len(train_loader))\n",
        "        mlflow.log_metric(\"train_loss\", running_loss / len(train_loader), step=epoch)\n",
        "        mlflow.log_metric(\"train_acc\", running_acc/tot_seen, step=epoch)\n",
        "        tot_corrette = 0\n",
        "        tot_eseguite = 0\n",
        "        running_test_loss = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "          for images, labels in test_loader:\n",
        "              images = torch.reshape(images, (images.shape[0], 1, 24, 24, 24))\n",
        "              images = images.type(torch.cuda.FloatTensor)\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              outputs = model(images)\n",
        "              test_loss = criterion(outputs, labels)\n",
        "              _, n_corrette=accuracy(outputs, labels)\n",
        "\n",
        "              running_test_loss += test_loss.item()\n",
        "              tot_corrette+=n_corrette.item()\n",
        "              tot_eseguite+=labels.shape[0]\n",
        "\n",
        "          test_acc=100* (tot_corrette/tot_eseguite)\n",
        "          val_loss = running_test_loss / len(test_loader)\n",
        "          acc_list_test.append(test_acc)\n",
        "          print(\"Test acc: \", test_acc)\n",
        "          print(\"Test loss: \", val_loss)\n",
        "\n",
        "          mlflow.log_metric(\"test_acc\", test_acc, step=epoch)\n",
        "          mlflow.log_metric(\"test_loss\", val_loss, step=epoch)\n",
        "\n",
        "\n",
        "        if val_loss < best_loss - min_delta:\n",
        "          print(\"MIGLIORATO\")\n",
        "          best_loss = val_loss\n",
        "          best_model_train_acc=running_acc/tot_seen\n",
        "          best_model_test_acc=test_acc\n",
        "          best_model_test_loss=val_loss\n",
        "          best_model_train_loss=running_loss / len(train_loader)\n",
        "          counter = 0\n",
        "          # Salva i pesi del modello se la validation loss è migliorata\n",
        "          torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "          counter += 1\n",
        "        # Verifica se raggiunto il criterio di early stopping\n",
        "          if counter >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              stop=True\n",
        "        print(\"BEST TEST LOSS: \", best_loss)\n",
        "\n",
        "    mlflow.set_tag(\"Epochs_stopped\", epoch+1)\n",
        "    mlflow.log_artifact(\"best_model.pt\")\n",
        "    mlflow.log_metric(\"best_test_acc\", best_model_test_acc)\n",
        "    mlflow.log_metric(\"best_test_loss\", best_model_test_loss)\n",
        "    mlflow.log_metric(\"best_train_acc\", best_model_train_acc)\n",
        "    mlflow.log_metric(\"best_train_loss\", best_model_train_loss)\n",
        "    mlflow.end_run()\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    PATH = './cnn.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    torch.cuda.empty_cache()\n",
        "  else:\n",
        "    print(\"RUN: \", [f\"params.{k} = '{v}'\" for k, v in expanded_grid[i].items()], \" già completata\" )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W6WnDqFXyrce",
        "outputId": "b6006344-b95a-4517-c669-dde0f1342a0c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: | Loss: 1.78818 | Acc: 0.266\n",
            "Test acc:  28.692021421439595\n",
            "Test loss:  1.752090917846371\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.752090917846371\n",
            "Epoch 1: | Loss: 1.74657 | Acc: 0.292\n",
            "Test acc:  29.862939094127256\n",
            "Test loss:  1.7322281254509282\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7322281254509282\n",
            "Epoch 2: | Loss: 1.68682 | Acc: 0.333\n",
            "Test acc:  35.581374239811204\n",
            "Test loss:  1.6520061196619376\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6520061196619376\n",
            "Epoch 3: | Loss: 1.61304 | Acc: 0.372\n",
            "Test acc:  37.7325950803304\n",
            "Test loss:  1.5967956354163286\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5967956354163286\n",
            "Epoch 4: | Loss: 1.56429 | Acc: 0.393\n",
            "Test acc:  40.038122900971224\n",
            "Test loss:  1.5489018535338386\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5489018535338386\n",
            "Epoch 5: | Loss: 1.51897 | Acc: 0.415\n",
            "Test acc:  41.04565671235363\n",
            "Test loss:  1.5419502389224278\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5419502389224278\n",
            "Epoch 6: | Loss: 1.48581 | Acc: 0.430\n",
            "Test acc:  41.20904057365889\n",
            "Test loss:  1.5284047423070566\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5284047423070566\n",
            "Epoch 7: | Loss: 1.45152 | Acc: 0.444\n",
            "Test acc:  43.6507216120541\n",
            "Test loss:  1.5049023524874208\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5049023524874208\n",
            "Epoch 8: | Loss: 1.42718 | Acc: 0.454\n",
            "Test acc:  43.32395388944359\n",
            "Test loss:  1.5071030462408341\n",
            "BEST TEST LOSS:  1.5049023524874208\n",
            "Epoch 9: | Loss: 1.39469 | Acc: 0.466\n",
            "Test acc:  42.48888082055006\n",
            "Test loss:  1.5043813438084774\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5043813438084774\n",
            "Epoch 10: | Loss: 1.37151 | Acc: 0.470\n",
            "Test acc:  43.28764636470909\n",
            "Test loss:  1.5038577042563113\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5038577042563113\n",
            "Epoch 11: | Loss: 1.33870 | Acc: 0.486\n",
            "Test acc:  43.46010710719796\n",
            "Test loss:  1.5035958393460753\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5035958393460753\n",
            "Epoch 12: | Loss: 1.30533 | Acc: 0.499\n",
            "Test acc:  42.96087864209858\n",
            "Test loss:  1.496835833340022\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.496835833340022\n",
            "Epoch 13: | Loss: 1.27668 | Acc: 0.510\n",
            "Test acc:  42.289189434510305\n",
            "Test loss:  1.5480055278436298\n",
            "BEST TEST LOSS:  1.496835833340022\n",
            "Epoch 14: | Loss: 1.24082 | Acc: 0.525\n",
            "Test acc:  43.45103022601434\n",
            "Test loss:  1.5541950223073795\n",
            "BEST TEST LOSS:  1.496835833340022\n",
            "Epoch 15: | Loss: 1.19720 | Acc: 0.538\n",
            "Test acc:  42.261958790959426\n",
            "Test loss:  1.5870117417649727\n",
            "Early stopping at epoch 16\n",
            "BEST TEST LOSS:  1.496835833340022\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.78642 | Acc: 0.267\n",
            "Test acc:  28.401561223563583\n",
            "Test loss:  1.75436250322816\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.75436250322816\n",
            "Epoch 1: | Loss: 1.73287 | Acc: 0.305\n",
            "Test acc:  32.70400290460198\n",
            "Test loss:  1.6939635428390063\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6939635428390063\n",
            "Epoch 2: | Loss: 1.64233 | Acc: 0.357\n",
            "Test acc:  36.97013706090587\n",
            "Test loss:  1.6039942661461803\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6039942661461803\n",
            "Epoch 3: | Loss: 1.59216 | Acc: 0.379\n",
            "Test acc:  38.69474448579468\n",
            "Test loss:  1.5726684049374795\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5726684049374795\n",
            "Epoch 4: | Loss: 1.54654 | Acc: 0.400\n",
            "Test acc:  39.49351002995371\n",
            "Test loss:  1.5623867766705553\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5623867766705553\n",
            "Epoch 5: | Loss: 1.50671 | Acc: 0.417\n",
            "Test acc:  40.482890078968865\n",
            "Test loss:  1.5338006495051302\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5338006495051302\n",
            "Epoch 6: | Loss: 1.47713 | Acc: 0.431\n",
            "Test acc:  40.755196514477625\n",
            "Test loss:  1.5432284181517673\n",
            "BEST TEST LOSS:  1.5338006495051302\n",
            "Epoch 7: | Loss: 1.44560 | Acc: 0.443\n",
            "Test acc:  43.078878097485706\n",
            "Test loss:  1.5054167963865865\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5054167963865865\n",
            "Epoch 8: | Loss: 1.41677 | Acc: 0.459\n",
            "Test acc:  42.724879731324314\n",
            "Test loss:  1.510772122124027\n",
            "BEST TEST LOSS:  1.5054167963865865\n",
            "Epoch 9: | Loss: 1.38605 | Acc: 0.463\n",
            "Test acc:  42.05319052373604\n",
            "Test loss:  1.5105957743749454\n",
            "BEST TEST LOSS:  1.5054167963865865\n",
            "Epoch 10: | Loss: 1.35756 | Acc: 0.474\n",
            "Test acc:  43.40564582009622\n",
            "Test loss:  1.5115431154394425\n",
            "Early stopping at epoch 11\n",
            "BEST TEST LOSS:  1.5054167963865865\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.79282 | Acc: 0.262\n",
            "Test acc:  28.247254243441954\n",
            "Test loss:  1.7659805234457027\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7659805234457027\n",
            "Epoch 1: | Loss: 1.74943 | Acc: 0.291\n",
            "Test acc:  31.79631478623945\n",
            "Test loss:  1.7068614752995486\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7068614752995486\n",
            "Epoch 2: | Loss: 1.67008 | Acc: 0.340\n",
            "Test acc:  35.31814468548607\n",
            "Test loss:  1.6534030589065112\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6534030589065112\n",
            "Epoch 3: | Loss: 1.59419 | Acc: 0.379\n",
            "Test acc:  38.25905418898067\n",
            "Test loss:  1.5866413054438684\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5866413054438684\n",
            "Epoch 4: | Loss: 1.55023 | Acc: 0.398\n",
            "Test acc:  39.80212399019697\n",
            "Test loss:  1.5632391430739034\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5632391430739034\n",
            "Epoch 5: | Loss: 1.51216 | Acc: 0.419\n",
            "Test acc:  41.10919488063901\n",
            "Test loss:  1.5404350020292867\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5404350020292867\n",
            "Epoch 6: | Loss: 1.47795 | Acc: 0.432\n",
            "Test acc:  42.479803939366434\n",
            "Test loss:  1.525826317037461\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.525826317037461\n",
            "Epoch 7: | Loss: 1.44740 | Acc: 0.439\n",
            "Test acc:  42.180266860306794\n",
            "Test loss:  1.5190829651893218\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5190829651893218\n",
            "Epoch 8: | Loss: 1.42966 | Acc: 0.450\n",
            "Test acc:  42.15303621675592\n",
            "Test loss:  1.5184640457175371\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5184640457175371\n",
            "Epoch 9: | Loss: 1.40123 | Acc: 0.459\n",
            "Test acc:  43.06072433511846\n",
            "Test loss:  1.4895541867768833\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.4895541867768833\n",
            "Epoch 10: | Loss: 1.37742 | Acc: 0.468\n",
            "Test acc:  42.9699555232822\n",
            "Test loss:  1.5168030999299418\n",
            "BEST TEST LOSS:  1.4895541867768833\n",
            "Epoch 11: | Loss: 1.35180 | Acc: 0.476\n",
            "Test acc:  43.13333938458746\n",
            "Test loss:  1.5218308371615548\n",
            "BEST TEST LOSS:  1.4895541867768833\n",
            "Epoch 12: | Loss: 1.33027 | Acc: 0.489\n",
            "Test acc:  43.29672324589271\n",
            "Test loss:  1.5232151491793593\n",
            "Early stopping at epoch 13\n",
            "BEST TEST LOSS:  1.4895541867768833\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.83513 | Acc: 0.246\n",
            "Test acc:  26.29572478896251\n",
            "Test loss:  1.792475845083336\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.792475845083336\n",
            "Epoch 1: | Loss: 1.78345 | Acc: 0.268\n",
            "Test acc:  26.713261323409277\n",
            "Test loss:  1.77575630466373\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.77575630466373\n",
            "Epoch 2: | Loss: 1.76911 | Acc: 0.277\n",
            "Test acc:  28.365253698829086\n",
            "Test loss:  1.7647838509840772\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7647838509840772\n",
            "Epoch 3: | Loss: 1.75615 | Acc: 0.285\n",
            "Test acc:  28.646637015521463\n",
            "Test loss:  1.7541480153971325\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7541480153971325\n",
            "Epoch 4: | Loss: 1.74268 | Acc: 0.297\n",
            "Test acc:  29.72678587637288\n",
            "Test loss:  1.7466074428117344\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7466074428117344\n",
            "Epoch 5: | Loss: 1.73589 | Acc: 0.301\n",
            "Test acc:  30.03539983661614\n",
            "Test loss:  1.7319282527603854\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7319282527603854\n",
            "Epoch 6: | Loss: 1.71935 | Acc: 0.309\n",
            "Test acc:  31.097394935100297\n",
            "Test loss:  1.7236236885103877\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7236236885103877\n",
            "Epoch 7: | Loss: 1.69523 | Acc: 0.326\n",
            "Test acc:  32.22292820186984\n",
            "Test loss:  1.7090183813447897\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7090183813447897\n",
            "Epoch 8: | Loss: 1.67376 | Acc: 0.339\n",
            "Test acc:  34.0019969138604\n",
            "Test loss:  1.6805932625180724\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6805932625180724\n",
            "Epoch 9: | Loss: 1.64215 | Acc: 0.356\n",
            "Test acc:  34.84614686393755\n",
            "Test loss:  1.657004824952583\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.657004824952583\n",
            "Epoch 10: | Loss: 1.61576 | Acc: 0.369\n",
            "Test acc:  35.95352636833984\n",
            "Test loss:  1.6376363383552242\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6376363383552242\n",
            "Epoch 11: | Loss: 1.58311 | Acc: 0.388\n",
            "Test acc:  36.46183171462285\n",
            "Test loss:  1.6263360446588153\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6263360446588153\n",
            "Epoch 12: | Loss: 1.55841 | Acc: 0.394\n",
            "Test acc:  37.351366070618134\n",
            "Test loss:  1.610514632539253\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.610514632539253\n",
            "Epoch 13: | Loss: 1.53429 | Acc: 0.406\n",
            "Test acc:  37.968593991104655\n",
            "Test loss:  1.5979592882828906\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5979592882828906\n",
            "Epoch 14: | Loss: 1.50884 | Acc: 0.416\n",
            "Test acc:  38.62212943632568\n",
            "Test loss:  1.5883717502472718\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5883717502472718\n",
            "Epoch 15: | Loss: 1.47664 | Acc: 0.431\n",
            "Test acc:  38.513206862122175\n",
            "Test loss:  1.5902148457620875\n",
            "BEST TEST LOSS:  1.5883717502472718\n",
            "Epoch 16: | Loss: 1.46014 | Acc: 0.438\n",
            "Test acc:  39.37551057456658\n",
            "Test loss:  1.5836984938968812\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5836984938968812\n",
            "Epoch 17: | Loss: 1.42791 | Acc: 0.453\n",
            "Test acc:  39.46627938640283\n",
            "Test loss:  1.5810542830153007\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5810542830153007\n",
            "Epoch 18: | Loss: 1.40041 | Acc: 0.465\n",
            "Test acc:  38.89443587183444\n",
            "Test loss:  1.586914808763934\n",
            "BEST TEST LOSS:  1.5810542830153007\n",
            "Epoch 19: | Loss: 1.36637 | Acc: 0.477\n",
            "Test acc:  39.2756648815467\n",
            "Test loss:  1.6039755861194147\n",
            "BEST TEST LOSS:  1.5810542830153007\n",
            "Epoch 20: | Loss: 1.34887 | Acc: 0.484\n",
            "Test acc:  39.302895525097576\n",
            "Test loss:  1.5943361810177048\n",
            "Early stopping at epoch 21\n",
            "BEST TEST LOSS:  1.5810542830153007\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.81636 | Acc: 0.253\n",
            "Test acc:  26.41372424434964\n",
            "Test loss:  1.7829389627269239\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7829389627269239\n",
            "Epoch 1: | Loss: 1.77398 | Acc: 0.278\n",
            "Test acc:  28.428791867114462\n",
            "Test loss:  1.7650052956763032\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7650052956763032\n",
            "Epoch 2: | Loss: 1.75557 | Acc: 0.286\n",
            "Test acc:  28.86448216392847\n",
            "Test loss:  1.754098870161641\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.754098870161641\n",
            "Epoch 3: | Loss: 1.74120 | Acc: 0.295\n",
            "Test acc:  29.49078696559862\n",
            "Test loss:  1.7411670030196966\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7411670030196966\n",
            "Epoch 4: | Loss: 1.72611 | Acc: 0.307\n",
            "Test acc:  31.514931469547065\n",
            "Test loss:  1.7204457611017834\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7204457611017834\n",
            "Epoch 5: | Loss: 1.69552 | Acc: 0.325\n",
            "Test acc:  32.65861849868385\n",
            "Test loss:  1.6935200381141178\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6935200381141178\n",
            "Epoch 6: | Loss: 1.66561 | Acc: 0.340\n",
            "Test acc:  34.7553780521013\n",
            "Test loss:  1.6582314189458858\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6582314189458858\n",
            "Epoch 7: | Loss: 1.62586 | Acc: 0.362\n",
            "Test acc:  35.44522102205682\n",
            "Test loss:  1.6421817120789104\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6421817120789104\n",
            "Epoch 8: | Loss: 1.59279 | Acc: 0.381\n",
            "Test acc:  36.988290823273125\n",
            "Test loss:  1.6162510751988846\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6162510751988846\n",
            "Epoch 9: | Loss: 1.56645 | Acc: 0.390\n",
            "Test acc:  37.66905691204502\n",
            "Test loss:  1.601481335011521\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.601481335011521\n",
            "Epoch 10: | Loss: 1.54450 | Acc: 0.403\n",
            "Test acc:  37.977670872288286\n",
            "Test loss:  1.5933199983111697\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5933199983111697\n",
            "Epoch 11: | Loss: 1.51261 | Acc: 0.418\n",
            "Test acc:  39.13951166379232\n",
            "Test loss:  1.5836221951280716\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5836221951280716\n",
            "Epoch 12: | Loss: 1.48616 | Acc: 0.427\n",
            "Test acc:  39.575201960606336\n",
            "Test loss:  1.5658565842347338\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5658565842347338\n",
            "Epoch 13: | Loss: 1.45561 | Acc: 0.443\n",
            "Test acc:  39.53889443587183\n",
            "Test loss:  1.5755936355260067\n",
            "BEST TEST LOSS:  1.5658565842347338\n",
            "Epoch 14: | Loss: 1.42282 | Acc: 0.453\n",
            "Test acc:  39.56612507942271\n",
            "Test loss:  1.5944821007678964\n",
            "BEST TEST LOSS:  1.5658565842347338\n",
            "Epoch 15: | Loss: 1.39897 | Acc: 0.466\n",
            "Test acc:  40.746119633294\n",
            "Test loss:  1.563388340045951\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.563388340045951\n",
            "Epoch 16: | Loss: 1.36474 | Acc: 0.478\n",
            "Test acc:  39.37551057456658\n",
            "Test loss:  1.6110544907564373\n",
            "BEST TEST LOSS:  1.563388340045951\n",
            "Epoch 17: | Loss: 1.34108 | Acc: 0.485\n",
            "Test acc:  39.865662158482344\n",
            "Test loss:  1.5735000020506753\n",
            "BEST TEST LOSS:  1.563388340045951\n",
            "Epoch 18: | Loss: 1.31263 | Acc: 0.497\n",
            "Test acc:  40.84596532631387\n",
            "Test loss:  1.589006164859485\n",
            "Early stopping at epoch 19\n",
            "BEST TEST LOSS:  1.563388340045951\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.80659 | Acc: 0.259\n",
            "Test acc:  27.07633657075429\n",
            "Test loss:  1.772415080511501\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.772415080511501\n",
            "Epoch 1: | Loss: 1.75981 | Acc: 0.284\n",
            "Test acc:  29.091404193519104\n",
            "Test loss:  1.7471939594070347\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7471939594070347\n",
            "Epoch 2: | Loss: 1.73639 | Acc: 0.300\n",
            "Test acc:  31.188163746936553\n",
            "Test loss:  1.7232101445942256\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7232101445942256\n",
            "Epoch 3: | Loss: 1.69289 | Acc: 0.326\n",
            "Test acc:  33.9656893891259\n",
            "Test loss:  1.665430060700874\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.665430060700874\n",
            "Epoch 4: | Loss: 1.64435 | Acc: 0.354\n",
            "Test acc:  36.39829354633748\n",
            "Test loss:  1.6285803524744993\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6285803524744993\n",
            "Epoch 5: | Loss: 1.60887 | Acc: 0.372\n",
            "Test acc:  37.32413542706726\n",
            "Test loss:  1.6054760693125643\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6054760693125643\n",
            "Epoch 6: | Loss: 1.56814 | Acc: 0.391\n",
            "Test acc:  37.90505582281928\n",
            "Test loss:  1.597604440126805\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.597604440126805\n",
            "Epoch 7: | Loss: 1.53408 | Acc: 0.405\n",
            "Test acc:  39.675047653626216\n",
            "Test loss:  1.562345355232327\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.562345355232327\n",
            "Epoch 8: | Loss: 1.49943 | Acc: 0.417\n",
            "Test acc:  40.210583643460104\n",
            "Test loss:  1.5525803559088294\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5525803559088294\n",
            "Epoch 9: | Loss: 1.46374 | Acc: 0.435\n",
            "Test acc:  40.59181265317237\n",
            "Test loss:  1.563628824460024\n",
            "BEST TEST LOSS:  1.5525803559088294\n",
            "Epoch 10: | Loss: 1.43643 | Acc: 0.450\n",
            "Test acc:  41.5358082962694\n",
            "Test loss:  1.545877037709848\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.545877037709848\n",
            "Epoch 11: | Loss: 1.40873 | Acc: 0.464\n",
            "Test acc:  40.90950349459925\n",
            "Test loss:  1.5443560105527756\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5443560105527756\n",
            "Epoch 12: | Loss: 1.38139 | Acc: 0.466\n",
            "Test acc:  41.154579286557144\n",
            "Test loss:  1.5431869491676375\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5431869491676375\n",
            "Epoch 13: | Loss: 1.34546 | Acc: 0.486\n",
            "Test acc:  41.59026958337115\n",
            "Test loss:  1.5442034784769048\n",
            "BEST TEST LOSS:  1.5431869491676375\n",
            "Epoch 14: | Loss: 1.31368 | Acc: 0.500\n",
            "Test acc:  39.45720250521921\n",
            "Test loss:  1.60560521431741\n",
            "BEST TEST LOSS:  1.5431869491676375\n",
            "Epoch 15: | Loss: 1.27763 | Acc: 0.512\n",
            "Test acc:  41.844422256512665\n",
            "Test loss:  1.5559712076462762\n",
            "Early stopping at epoch 16\n",
            "BEST TEST LOSS:  1.5431869491676375\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.94114 | Acc: 0.162\n",
            "Test acc:  20.913134247072705\n",
            "Test loss:  1.9026518467533795\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9026518467533795\n",
            "Epoch 1: | Loss: 1.88363 | Acc: 0.224\n",
            "Test acc:  23.527276027956795\n",
            "Test loss:  1.8662951089054174\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8662951089054174\n",
            "Epoch 2: | Loss: 1.85285 | Acc: 0.243\n",
            "Test acc:  24.4258872651357\n",
            "Test loss:  1.8475796824934854\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8475796824934854\n",
            "Epoch 3: | Loss: 1.83786 | Acc: 0.252\n",
            "Test acc:  24.788962512480712\n",
            "Test loss:  1.828567313321064\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.828567313321064\n",
            "Epoch 4: | Loss: 1.82545 | Acc: 0.256\n",
            "Test acc:  25.515113007170736\n",
            "Test loss:  1.8173061009776386\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8173061009776386\n",
            "Epoch 5: | Loss: 1.81426 | Acc: 0.257\n",
            "Test acc:  25.53326676953799\n",
            "Test loss:  1.809257420501268\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.809257420501268\n",
            "Epoch 6: | Loss: 1.80953 | Acc: 0.260\n",
            "Test acc:  25.84188072978125\n",
            "Test loss:  1.8066176389683188\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8066176389683188\n",
            "Epoch 7: | Loss: 1.80309 | Acc: 0.261\n",
            "Test acc:  26.168648452391757\n",
            "Test loss:  1.7991260965435492\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7991260965435492\n",
            "Epoch 8: | Loss: 1.79497 | Acc: 0.266\n",
            "Test acc:  25.905418898066625\n",
            "Test loss:  1.7986623379536446\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7986623379536446\n",
            "Epoch 9: | Loss: 1.79227 | Acc: 0.268\n",
            "Test acc:  26.431878006716893\n",
            "Test loss:  1.7904692846915626\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7904692846915626\n",
            "Epoch 10: | Loss: 1.78944 | Acc: 0.269\n",
            "Test acc:  26.32295543251339\n",
            "Test loss:  1.7918137098323403\n",
            "BEST TEST LOSS:  1.7904692846915626\n",
            "Epoch 11: | Loss: 1.78874 | Acc: 0.267\n",
            "Test acc:  26.65880003630752\n",
            "Test loss:  1.7877615042504547\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7877615042504547\n",
            "Epoch 12: | Loss: 1.78219 | Acc: 0.272\n",
            "Test acc:  26.94018335299991\n",
            "Test loss:  1.7825559384560998\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7825559384560998\n",
            "Epoch 13: | Loss: 1.77800 | Acc: 0.277\n",
            "Test acc:  26.540800580920397\n",
            "Test loss:  1.781117978123571\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.781117978123571\n",
            "Epoch 14: | Loss: 1.77735 | Acc: 0.277\n",
            "Test acc:  27.394027412181178\n",
            "Test loss:  1.7766396840872791\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7766396840872791\n",
            "Epoch 15: | Loss: 1.77649 | Acc: 0.275\n",
            "Test acc:  27.167105382590538\n",
            "Test loss:  1.775725988294348\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.775725988294348\n",
            "Epoch 16: | Loss: 1.77348 | Acc: 0.274\n",
            "Test acc:  27.375873649813926\n",
            "Test loss:  1.7765117621835256\n",
            "BEST TEST LOSS:  1.775725988294348\n",
            "Epoch 17: | Loss: 1.76912 | Acc: 0.279\n",
            "Test acc:  27.838794590178818\n",
            "Test loss:  1.7737106746331806\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7737106746331806\n",
            "Epoch 18: | Loss: 1.76801 | Acc: 0.281\n",
            "Test acc:  27.41218117454843\n",
            "Test loss:  1.7723737493415788\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7723737493415788\n",
            "Epoch 19: | Loss: 1.76524 | Acc: 0.281\n",
            "Test acc:  27.793410184260686\n",
            "Test loss:  1.7674760577306583\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7674760577306583\n",
            "Epoch 20: | Loss: 1.76501 | Acc: 0.280\n",
            "Test acc:  27.902332758464194\n",
            "Test loss:  1.7682840252198235\n",
            "BEST TEST LOSS:  1.7674760577306583\n",
            "Epoch 21: | Loss: 1.76301 | Acc: 0.287\n",
            "Test acc:  27.856948352546063\n",
            "Test loss:  1.7689046418735748\n",
            "BEST TEST LOSS:  1.7674760577306583\n",
            "Epoch 22: | Loss: 1.76198 | Acc: 0.285\n",
            "Test acc:  27.86602523372969\n",
            "Test loss:  1.7628456967414459\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7628456967414459\n",
            "Epoch 23: | Loss: 1.75898 | Acc: 0.285\n",
            "Test acc:  28.229100481074703\n",
            "Test loss:  1.7644344650941088\n",
            "BEST TEST LOSS:  1.7628456967414459\n",
            "Epoch 24: | Loss: 1.75899 | Acc: 0.287\n",
            "Test acc:  28.637560134337843\n",
            "Test loss:  1.7586419410099183\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7586419410099183\n",
            "Epoch 25: | Loss: 1.75517 | Acc: 0.290\n",
            "Test acc:  28.25633112462558\n",
            "Test loss:  1.759982412950152\n",
            "BEST TEST LOSS:  1.7586419410099183\n",
            "Epoch 26: | Loss: 1.75420 | Acc: 0.290\n",
            "Test acc:  28.38340746119633\n",
            "Test loss:  1.7599428967933435\n",
            "BEST TEST LOSS:  1.7586419410099183\n",
            "Epoch 27: | Loss: 1.75312 | Acc: 0.291\n",
            "Test acc:  28.238177362258327\n",
            "Test loss:  1.7587461230382755\n",
            "Early stopping at epoch 28\n",
            "BEST TEST LOSS:  1.7586419410099183\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.91206 | Acc: 0.193\n",
            "Test acc:  23.427430334936915\n",
            "Test loss:  1.8722985238698178\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8722985238698178\n",
            "Epoch 1: | Loss: 1.85432 | Acc: 0.240\n",
            "Test acc:  24.816193156031588\n",
            "Test loss:  1.8356953521684416\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8356953521684416\n",
            "Epoch 2: | Loss: 1.82502 | Acc: 0.253\n",
            "Test acc:  25.2155759281111\n",
            "Test loss:  1.8163050192628982\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8163050192628982\n",
            "Epoch 3: | Loss: 1.81130 | Acc: 0.257\n",
            "Test acc:  26.150494690024505\n",
            "Test loss:  1.8052535077740002\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8052535077740002\n",
            "Epoch 4: | Loss: 1.79860 | Acc: 0.263\n",
            "Test acc:  26.250340383044385\n",
            "Test loss:  1.7959319332431507\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7959319332431507\n",
            "Epoch 5: | Loss: 1.79501 | Acc: 0.260\n",
            "Test acc:  26.24126350186076\n",
            "Test loss:  1.7904799080997533\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7904799080997533\n",
            "Epoch 6: | Loss: 1.78737 | Acc: 0.266\n",
            "Test acc:  26.568031224471273\n",
            "Test loss:  1.7870718723087642\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7870718723087642\n",
            "Epoch 7: | Loss: 1.78271 | Acc: 0.271\n",
            "Test acc:  26.912952709449034\n",
            "Test loss:  1.7845151114325992\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7845151114325992\n",
            "Epoch 8: | Loss: 1.78004 | Acc: 0.270\n",
            "Test acc:  26.504493056185897\n",
            "Test loss:  1.7819316352722963\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7819316352722963\n",
            "Epoch 9: | Loss: 1.77725 | Acc: 0.273\n",
            "Test acc:  27.030952164836165\n",
            "Test loss:  1.7806317923385973\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7806317923385973\n",
            "Epoch 10: | Loss: 1.77466 | Acc: 0.273\n",
            "Test acc:  26.804030135245533\n",
            "Test loss:  1.7775934569408438\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7775934569408438\n",
            "Epoch 11: | Loss: 1.77337 | Acc: 0.275\n",
            "Test acc:  27.067259689570662\n",
            "Test loss:  1.7741725672187143\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7741725672187143\n",
            "Epoch 12: | Loss: 1.76793 | Acc: 0.278\n",
            "Test acc:  27.357719887446674\n",
            "Test loss:  1.7727325079758043\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7727325079758043\n",
            "Epoch 13: | Loss: 1.76512 | Acc: 0.279\n",
            "Test acc:  27.321412362712174\n",
            "Test loss:  1.7688737708020073\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7688737708020073\n",
            "Epoch 14: | Loss: 1.76613 | Acc: 0.282\n",
            "Test acc:  27.96587092674957\n",
            "Test loss:  1.7669606994342253\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7669606994342253\n",
            "Epoch 15: | Loss: 1.76128 | Acc: 0.284\n",
            "Test acc:  27.738948897158938\n",
            "Test loss:  1.766687598531646\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.766687598531646\n",
            "Epoch 16: | Loss: 1.76102 | Acc: 0.283\n",
            "Test acc:  28.0838703821367\n",
            "Test loss:  1.763184739675136\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.763184739675136\n",
            "Epoch 17: | Loss: 1.75658 | Acc: 0.287\n",
            "Test acc:  27.984024689116822\n",
            "Test loss:  1.7626002537721843\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7626002537721843\n",
            "Epoch 18: | Loss: 1.75356 | Acc: 0.287\n",
            "Test acc:  28.283561768176458\n",
            "Test loss:  1.7601299912943316\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7601299912943316\n",
            "Epoch 19: | Loss: 1.75207 | Acc: 0.287\n",
            "Test acc:  28.18371607515658\n",
            "Test loss:  1.759358924937386\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.759358924937386\n",
            "Epoch 20: | Loss: 1.75012 | Acc: 0.289\n",
            "Test acc:  28.610329490786967\n",
            "Test loss:  1.7579740120496363\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7579740120496363\n",
            "Epoch 21: | Loss: 1.74883 | Acc: 0.291\n",
            "Test acc:  28.546791322501587\n",
            "Test loss:  1.7551329425304611\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7551329425304611\n",
            "Epoch 22: | Loss: 1.74824 | Acc: 0.291\n",
            "Test acc:  28.8372515203776\n",
            "Test loss:  1.7543377214773541\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7543377214773541\n",
            "Epoch 23: | Loss: 1.74480 | Acc: 0.290\n",
            "Test acc:  28.746482708541343\n",
            "Test loss:  1.7498168352711407\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7498168352711407\n",
            "Epoch 24: | Loss: 1.74362 | Acc: 0.296\n",
            "Test acc:  29.28201869837524\n",
            "Test loss:  1.7488488523946333\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7488488523946333\n",
            "Epoch 25: | Loss: 1.73964 | Acc: 0.294\n",
            "Test acc:  28.592175728419715\n",
            "Test loss:  1.7509184999962073\n",
            "BEST TEST LOSS:  1.7488488523946333\n",
            "Epoch 26: | Loss: 1.73871 | Acc: 0.295\n",
            "Test acc:  29.018789144050107\n",
            "Test loss:  1.7480170464929128\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7480170464929128\n",
            "Epoch 27: | Loss: 1.73506 | Acc: 0.298\n",
            "Test acc:  29.463556322047747\n",
            "Test loss:  1.7448035795564596\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7448035795564596\n",
            "Epoch 28: | Loss: 1.73373 | Acc: 0.302\n",
            "Test acc:  29.127711718253607\n",
            "Test loss:  1.7460002285896699\n",
            "BEST TEST LOSS:  1.7448035795564596\n",
            "Epoch 29: | Loss: 1.72911 | Acc: 0.304\n",
            "Test acc:  29.009712262866476\n",
            "Test loss:  1.7480932549934167\n",
            "BEST TEST LOSS:  1.7448035795564596\n",
            "Epoch 30: | Loss: 1.72590 | Acc: 0.302\n",
            "Test acc:  29.018789144050107\n",
            "Test loss:  1.7453377053916799\n",
            "Early stopping at epoch 31\n",
            "BEST TEST LOSS:  1.7448035795564596\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.88951 | Acc: 0.232\n",
            "Test acc:  25.29726785876373\n",
            "Test loss:  1.8411109399244276\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8411109399244276\n",
            "Epoch 1: | Loss: 1.82520 | Acc: 0.257\n",
            "Test acc:  26.096033402922757\n",
            "Test loss:  1.8103986709793178\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8103986709793178\n",
            "Epoch 2: | Loss: 1.80363 | Acc: 0.263\n",
            "Test acc:  26.159571571208133\n",
            "Test loss:  1.7933467144221928\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7933467144221928\n",
            "Epoch 3: | Loss: 1.79046 | Acc: 0.267\n",
            "Test acc:  27.276027956794046\n",
            "Test loss:  1.783289254056236\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.783289254056236\n",
            "Epoch 4: | Loss: 1.78173 | Acc: 0.271\n",
            "Test acc:  27.003721521285286\n",
            "Test loss:  1.7767466671894052\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7767466671894052\n",
            "Epoch 5: | Loss: 1.77704 | Acc: 0.277\n",
            "Test acc:  27.28510483797767\n",
            "Test loss:  1.7726009140124899\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7726009140124899\n",
            "Epoch 6: | Loss: 1.77017 | Acc: 0.277\n",
            "Test acc:  27.648180085322682\n",
            "Test loss:  1.7663845140809957\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7663845140809957\n",
            "Epoch 7: | Loss: 1.76709 | Acc: 0.278\n",
            "Test acc:  27.993101570300443\n",
            "Test loss:  1.7670714400407206\n",
            "BEST TEST LOSS:  1.7663845140809957\n",
            "Epoch 8: | Loss: 1.76113 | Acc: 0.286\n",
            "Test acc:  28.62848325315422\n",
            "Test loss:  1.7630312663282273\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7630312663282273\n",
            "Epoch 9: | Loss: 1.75533 | Acc: 0.286\n",
            "Test acc:  28.437868748298083\n",
            "Test loss:  1.75930585957676\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.75930585957676\n",
            "Epoch 10: | Loss: 1.75425 | Acc: 0.290\n",
            "Test acc:  28.356176817645455\n",
            "Test loss:  1.7561101148583296\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7561101148583296\n",
            "Epoch 11: | Loss: 1.75073 | Acc: 0.293\n",
            "Test acc:  28.982481619315603\n",
            "Test loss:  1.751879080871626\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.751879080871626\n",
            "Epoch 12: | Loss: 1.74459 | Acc: 0.294\n",
            "Test acc:  29.30924934192611\n",
            "Test loss:  1.7506850654679227\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7506850654679227\n",
            "Epoch 13: | Loss: 1.74100 | Acc: 0.297\n",
            "Test acc:  29.40909503494599\n",
            "Test loss:  1.746191489214153\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.746191489214153\n",
            "Epoch 14: | Loss: 1.73729 | Acc: 0.300\n",
            "Test acc:  29.25478805482436\n",
            "Test loss:  1.7437947898930897\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7437947898930897\n",
            "Epoch 15: | Loss: 1.73459 | Acc: 0.305\n",
            "Test acc:  29.363710629027867\n",
            "Test loss:  1.7426826395740398\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7426826395740398\n",
            "Epoch 16: | Loss: 1.72701 | Acc: 0.307\n",
            "Test acc:  29.817554688209132\n",
            "Test loss:  1.7372892957202273\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7372892957202273\n",
            "Epoch 17: | Loss: 1.72320 | Acc: 0.310\n",
            "Test acc:  30.289552509757645\n",
            "Test loss:  1.7341166204110736\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7341166204110736\n",
            "Epoch 18: | Loss: 1.72195 | Acc: 0.310\n",
            "Test acc:  30.4892438957974\n",
            "Test loss:  1.7318442357068806\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7318442357068806\n",
            "Epoch 19: | Loss: 1.71421 | Acc: 0.316\n",
            "Test acc:  30.661704638286285\n",
            "Test loss:  1.7309765994893334\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7309765994893334\n",
            "Epoch 20: | Loss: 1.71113 | Acc: 0.317\n",
            "Test acc:  31.48770082599619\n",
            "Test loss:  1.7250437950123252\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7250437950123252\n",
            "Epoch 21: | Loss: 1.70323 | Acc: 0.322\n",
            "Test acc:  31.160933103385673\n",
            "Test loss:  1.7220088408861547\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7220088408861547\n",
            "Epoch 22: | Loss: 1.69052 | Acc: 0.334\n",
            "Test acc:  31.632930924934193\n",
            "Test loss:  1.7139411324021445\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7139411324021445\n",
            "Epoch 23: | Loss: 1.68580 | Acc: 0.333\n",
            "Test acc:  32.16846691476808\n",
            "Test loss:  1.7079197278601586\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7079197278601586\n",
            "Epoch 24: | Loss: 1.67792 | Acc: 0.336\n",
            "Test acc:  32.68584914223473\n",
            "Test loss:  1.6985361300451907\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6985361300451907\n",
            "Epoch 25: | Loss: 1.66748 | Acc: 0.346\n",
            "Test acc:  33.430153399292\n",
            "Test loss:  1.6942837272765319\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6942837272765319\n",
            "Epoch 26: | Loss: 1.65696 | Acc: 0.354\n",
            "Test acc:  33.63892166651539\n",
            "Test loss:  1.6859305242582552\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6859305242582552\n",
            "Epoch 27: | Loss: 1.64566 | Acc: 0.355\n",
            "Test acc:  34.70091676499955\n",
            "Test loss:  1.670038962639825\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.670038962639825\n",
            "Epoch 28: | Loss: 1.63553 | Acc: 0.360\n",
            "Test acc:  34.51030226014342\n",
            "Test loss:  1.6687889050886122\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6687889050886122\n",
            "Epoch 29: | Loss: 1.61908 | Acc: 0.371\n",
            "Test acc:  35.32722156666969\n",
            "Test loss:  1.6532358633989543\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6532358633989543\n",
            "Epoch 30: | Loss: 1.60406 | Acc: 0.379\n",
            "Test acc:  34.97322320050831\n",
            "Test loss:  1.65374846816752\n",
            "BEST TEST LOSS:  1.6532358633989543\n",
            "Epoch 31: | Loss: 1.59051 | Acc: 0.385\n",
            "Test acc:  35.653989289280204\n",
            "Test loss:  1.6447710935780078\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6447710935780078\n",
            "Epoch 32: | Loss: 1.57973 | Acc: 0.391\n",
            "Test acc:  35.77198874466733\n",
            "Test loss:  1.637458870865706\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.637458870865706\n",
            "Epoch 33: | Loss: 1.56221 | Acc: 0.396\n",
            "Test acc:  36.56167740764273\n",
            "Test loss:  1.6310047841485524\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6310047841485524\n",
            "Epoch 34: | Loss: 1.55447 | Acc: 0.401\n",
            "Test acc:  37.41490423890351\n",
            "Test loss:  1.6177046547046285\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6177046547046285\n",
            "Epoch 35: | Loss: 1.53532 | Acc: 0.408\n",
            "Test acc:  37.02459834800763\n",
            "Test loss:  1.6180005424973594\n",
            "BEST TEST LOSS:  1.6177046547046285\n",
            "Epoch 36: | Loss: 1.52522 | Acc: 0.414\n",
            "Test acc:  37.37859671416901\n",
            "Test loss:  1.6044440972322673\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6044440972322673\n",
            "Epoch 37: | Loss: 1.50928 | Acc: 0.421\n",
            "Test acc:  37.678133793228646\n",
            "Test loss:  1.604640556897731\n",
            "BEST TEST LOSS:  1.6044440972322673\n",
            "Epoch 38: | Loss: 1.49418 | Acc: 0.428\n",
            "Test acc:  38.195516020695294\n",
            "Test loss:  1.6013665605831697\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6013665605831697\n",
            "Epoch 39: | Loss: 1.47661 | Acc: 0.434\n",
            "Test acc:  38.10474720885904\n",
            "Test loss:  1.6033046259356372\n",
            "BEST TEST LOSS:  1.6013665605831697\n",
            "Epoch 40: | Loss: 1.46440 | Acc: 0.436\n",
            "Test acc:  38.46782245620405\n",
            "Test loss:  1.5945780752711214\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5945780752711214\n",
            "Epoch 41: | Loss: 1.45043 | Acc: 0.445\n",
            "Test acc:  38.15013161477716\n",
            "Test loss:  1.5961525075008414\n",
            "BEST TEST LOSS:  1.5945780752711214\n",
            "Epoch 42: | Loss: 1.44111 | Acc: 0.446\n",
            "Test acc:  38.97612780248706\n",
            "Test loss:  1.5941487771238205\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5941487771238205\n",
            "Epoch 43: | Loss: 1.41965 | Acc: 0.459\n",
            "Test acc:  38.83089770354906\n",
            "Test loss:  1.6005529206612206\n",
            "BEST TEST LOSS:  1.5941487771238205\n",
            "Epoch 44: | Loss: 1.40769 | Acc: 0.461\n",
            "Test acc:  39.502586911137335\n",
            "Test loss:  1.5985429183596132\n",
            "BEST TEST LOSS:  1.5941487771238205\n",
            "Epoch 45: | Loss: 1.39600 | Acc: 0.468\n",
            "Test acc:  38.93074339656894\n",
            "Test loss:  1.5943341751319136\n",
            "Early stopping at epoch 46\n",
            "BEST TEST LOSS:  1.5941487771238205\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.78829 | Acc: 0.266\n",
            "Test acc:  28.819097758010347\n",
            "Test loss:  1.7588759963912082\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7588759963912082\n",
            "Epoch 1: | Loss: 1.73153 | Acc: 0.300\n",
            "Test acc:  32.51338839974585\n",
            "Test loss:  1.6849837337615172\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6849837337615172\n",
            "Epoch 2: | Loss: 1.66157 | Acc: 0.346\n",
            "Test acc:  35.44522102205682\n",
            "Test loss:  1.6352648300931634\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6352648300931634\n",
            "Epoch 3: | Loss: 1.59647 | Acc: 0.377\n",
            "Test acc:  38.23182354542979\n",
            "Test loss:  1.5890181843255986\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5890181843255986\n",
            "Epoch 4: | Loss: 1.55389 | Acc: 0.395\n",
            "Test acc:  40.30135245529636\n",
            "Test loss:  1.5546829652235\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5546829652235\n",
            "Epoch 5: | Loss: 1.51311 | Acc: 0.413\n",
            "Test acc:  40.673504583825\n",
            "Test loss:  1.5434669938390655\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5434669938390655\n",
            "Epoch 6: | Loss: 1.48993 | Acc: 0.424\n",
            "Test acc:  41.24534809839339\n",
            "Test loss:  1.5338123740488394\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5338123740488394\n",
            "Epoch 7: | Loss: 1.45751 | Acc: 0.439\n",
            "Test acc:  42.062267404919666\n",
            "Test loss:  1.5303397550748263\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5303397550748263\n",
            "Epoch 8: | Loss: 1.42297 | Acc: 0.453\n",
            "Test acc:  42.17118997912318\n",
            "Test loss:  1.5118113556349209\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5118113556349209\n",
            "Epoch 9: | Loss: 1.39227 | Acc: 0.464\n",
            "Test acc:  42.625034038304435\n",
            "Test loss:  1.5097061237158802\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5097061237158802\n",
            "Epoch 10: | Loss: 1.36317 | Acc: 0.475\n",
            "Test acc:  43.024416810383954\n",
            "Test loss:  1.521665427037057\n",
            "BEST TEST LOSS:  1.5097061237158802\n",
            "Epoch 11: | Loss: 1.32122 | Acc: 0.492\n",
            "Test acc:  43.15149314695471\n",
            "Test loss:  1.5336980392478106\n",
            "BEST TEST LOSS:  1.5097061237158802\n",
            "Epoch 12: | Loss: 1.29606 | Acc: 0.501\n",
            "Test acc:  42.77026413724244\n",
            "Test loss:  1.5291230954186765\n",
            "Early stopping at epoch 13\n",
            "BEST TEST LOSS:  1.5097061237158802\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.78703 | Acc: 0.271\n",
            "Test acc:  28.474176273032587\n",
            "Test loss:  1.761417396495797\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.761417396495797\n",
            "Epoch 1: | Loss: 1.74633 | Acc: 0.292\n",
            "Test acc:  30.353090678043028\n",
            "Test loss:  1.7247265542862733\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7247265542862733\n",
            "Epoch 2: | Loss: 1.67214 | Acc: 0.335\n",
            "Test acc:  36.36198602160297\n",
            "Test loss:  1.6205468818631474\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6205468818631474\n",
            "Epoch 3: | Loss: 1.60817 | Acc: 0.372\n",
            "Test acc:  37.34228918943451\n",
            "Test loss:  1.6072866069099117\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6072866069099117\n",
            "Epoch 4: | Loss: 1.55869 | Acc: 0.395\n",
            "Test acc:  40.18335299990923\n",
            "Test loss:  1.5716321833560922\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5716321833560922\n",
            "Epoch 5: | Loss: 1.52186 | Acc: 0.413\n",
            "Test acc:  41.644730870472905\n",
            "Test loss:  1.5290500931657118\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5290500931657118\n",
            "Epoch 6: | Loss: 1.47932 | Acc: 0.430\n",
            "Test acc:  41.21811745484252\n",
            "Test loss:  1.5264824876895529\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5264824876895529\n",
            "Epoch 7: | Loss: 1.45966 | Acc: 0.440\n",
            "Test acc:  42.38903512753018\n",
            "Test loss:  1.5119144544436063\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5119144544436063\n",
            "Epoch 8: | Loss: 1.42447 | Acc: 0.457\n",
            "Test acc:  41.935191068348914\n",
            "Test loss:  1.5079976340938854\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5079976340938854\n",
            "Epoch 9: | Loss: 1.39282 | Acc: 0.466\n",
            "Test acc:  42.69764908777344\n",
            "Test loss:  1.5125161685006467\n",
            "BEST TEST LOSS:  1.5079976340938854\n",
            "Epoch 10: | Loss: 1.36402 | Acc: 0.473\n",
            "Test acc:  42.94272487973132\n",
            "Test loss:  1.5007013330569845\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5007013330569845\n",
            "Epoch 11: | Loss: 1.33044 | Acc: 0.486\n",
            "Test acc:  43.43287646364709\n",
            "Test loss:  1.4992728095523196\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.4992728095523196\n",
            "Epoch 12: | Loss: 1.30663 | Acc: 0.497\n",
            "Test acc:  43.36026141417808\n",
            "Test loss:  1.5265170680305171\n",
            "BEST TEST LOSS:  1.4992728095523196\n",
            "Epoch 13: | Loss: 1.27358 | Acc: 0.512\n",
            "Test acc:  43.17872379050558\n",
            "Test loss:  1.5300278263974052\n",
            "BEST TEST LOSS:  1.4992728095523196\n",
            "Epoch 14: | Loss: 1.23329 | Acc: 0.528\n",
            "Test acc:  43.99564309703186\n",
            "Test loss:  1.5565962688082216\n",
            "Early stopping at epoch 15\n",
            "BEST TEST LOSS:  1.4992728095523196\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.79039 | Acc: 0.269\n",
            "Test acc:  26.94018335299991\n",
            "Test loss:  1.777229685314818\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.777229685314818\n",
            "Epoch 1: | Loss: 1.72798 | Acc: 0.306\n",
            "Test acc:  33.61169102296451\n",
            "Test loss:  1.6729858403950069\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6729858403950069\n",
            "Epoch 2: | Loss: 1.64968 | Acc: 0.352\n",
            "Test acc:  36.84306072433512\n",
            "Test loss:  1.6081195883668227\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6081195883668227\n",
            "Epoch 3: | Loss: 1.59992 | Acc: 0.376\n",
            "Test acc:  39.330126168648455\n",
            "Test loss:  1.572725353213404\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.572725353213404\n",
            "Epoch 4: | Loss: 1.55071 | Acc: 0.400\n",
            "Test acc:  40.019969138603976\n",
            "Test loss:  1.5604328399448726\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5604328399448726\n",
            "Epoch 5: | Loss: 1.51269 | Acc: 0.415\n",
            "Test acc:  42.207497503857674\n",
            "Test loss:  1.5263551645885314\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5263551645885314\n",
            "Epoch 6: | Loss: 1.48725 | Acc: 0.427\n",
            "Test acc:  40.96396478170101\n",
            "Test loss:  1.5467239501159316\n",
            "BEST TEST LOSS:  1.5263551645885314\n",
            "Epoch 7: | Loss: 1.46104 | Acc: 0.438\n",
            "Test acc:  42.19842062267405\n",
            "Test loss:  1.5192164616777717\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5192164616777717\n",
            "Epoch 8: | Loss: 1.42635 | Acc: 0.453\n",
            "Test acc:  42.606880275937186\n",
            "Test loss:  1.503554885787082\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.503554885787082\n",
            "Epoch 9: | Loss: 1.40450 | Acc: 0.458\n",
            "Test acc:  43.224108196423714\n",
            "Test loss:  1.5157121178731754\n",
            "BEST TEST LOSS:  1.503554885787082\n",
            "Epoch 10: | Loss: 1.38064 | Acc: 0.468\n",
            "Test acc:  43.1151856222202\n",
            "Test loss:  1.5216061958687843\n",
            "BEST TEST LOSS:  1.503554885787082\n",
            "Epoch 11: | Loss: 1.35048 | Acc: 0.482\n",
            "Test acc:  43.47826086956522\n",
            "Test loss:  1.513398599762448\n",
            "Early stopping at epoch 12\n",
            "BEST TEST LOSS:  1.503554885787082\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.83459 | Acc: 0.247\n",
            "Test acc:  25.805573205046745\n",
            "Test loss:  1.795910792543709\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.795910792543709\n",
            "Epoch 1: | Loss: 1.78329 | Acc: 0.269\n",
            "Test acc:  27.72987201597531\n",
            "Test loss:  1.773351477749775\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.773351477749775\n",
            "Epoch 2: | Loss: 1.76607 | Acc: 0.279\n",
            "Test acc:  28.474176273032587\n",
            "Test loss:  1.7602005197822703\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7602005197822703\n",
            "Epoch 3: | Loss: 1.74945 | Acc: 0.291\n",
            "Test acc:  29.28201869837524\n",
            "Test loss:  1.7475972161816724\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7475972161816724\n",
            "Epoch 4: | Loss: 1.73555 | Acc: 0.304\n",
            "Test acc:  30.625397113551784\n",
            "Test loss:  1.7304718770043699\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7304718770043699\n",
            "Epoch 5: | Loss: 1.71481 | Acc: 0.319\n",
            "Test acc:  31.333393845874557\n",
            "Test loss:  1.7160614570441273\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7160614570441273\n",
            "Epoch 6: | Loss: 1.69172 | Acc: 0.331\n",
            "Test acc:  33.40292275574113\n",
            "Test loss:  1.6878381726369693\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6878381726369693\n",
            "Epoch 7: | Loss: 1.65396 | Acc: 0.350\n",
            "Test acc:  34.63737859671417\n",
            "Test loss:  1.659079782535575\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.659079782535575\n",
            "Epoch 8: | Loss: 1.61829 | Acc: 0.368\n",
            "Test acc:  35.95352636833984\n",
            "Test loss:  1.6353328041947646\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6353328041947646\n",
            "Epoch 9: | Loss: 1.58931 | Acc: 0.380\n",
            "Test acc:  36.643369338295365\n",
            "Test loss:  1.6176988906253968\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6176988906253968\n",
            "Epoch 10: | Loss: 1.56352 | Acc: 0.394\n",
            "Test acc:  37.81428701098303\n",
            "Test loss:  1.6033251189083033\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6033251189083033\n",
            "Epoch 11: | Loss: 1.54221 | Acc: 0.402\n",
            "Test acc:  38.068439684124534\n",
            "Test loss:  1.6034401427803702\n",
            "BEST TEST LOSS:  1.6033251189083033\n",
            "Epoch 12: | Loss: 1.51161 | Acc: 0.415\n",
            "Test acc:  38.72197512934556\n",
            "Test loss:  1.5768815368586193\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5768815368586193\n",
            "Epoch 13: | Loss: 1.49094 | Acc: 0.425\n",
            "Test acc:  38.38613052555142\n",
            "Test loss:  1.5900154244693028\n",
            "BEST TEST LOSS:  1.5768815368586193\n",
            "Epoch 14: | Loss: 1.47648 | Acc: 0.432\n",
            "Test acc:  39.72950894072797\n",
            "Test loss:  1.5679361392996904\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5679361392996904\n",
            "Epoch 15: | Loss: 1.43802 | Acc: 0.447\n",
            "Test acc:  39.90196968321685\n",
            "Test loss:  1.5652818617793176\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.5652818617793176\n",
            "Epoch 16: | Loss: 1.42592 | Acc: 0.455\n",
            "Test acc:  39.85658527729872\n",
            "Test loss:  1.5715492067998544\n",
            "BEST TEST LOSS:  1.5652818617793176\n",
            "Epoch 17: | Loss: 1.40308 | Acc: 0.461\n",
            "Test acc:  40.59181265317237\n",
            "Test loss:  1.5655613014463745\n",
            "BEST TEST LOSS:  1.5652818617793176\n",
            "Epoch 18: | Loss: 1.36979 | Acc: 0.475\n",
            "Test acc:  40.310429336479984\n",
            "Test loss:  1.6031495539439207\n",
            "Early stopping at epoch 19\n",
            "BEST TEST LOSS:  1.5652818617793176\n",
            "Finished Training\n",
            "Epoch 0: | Loss: 1.82822 | Acc: 0.245\n",
            "Test acc:  26.54987746210402\n",
            "Test loss:  1.7824311118594485\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7824311118594485\n",
            "Epoch 1: | Loss: 1.77357 | Acc: 0.273\n",
            "Test acc:  27.466642461650174\n",
            "Test loss:  1.7642080997456016\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7642080997456016\n",
            "Epoch 2: | Loss: 1.75710 | Acc: 0.284\n",
            "Test acc:  28.8372515203776\n",
            "Test loss:  1.7498225779891703\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7498225779891703\n",
            "Epoch 3: | Loss: 1.74334 | Acc: 0.292\n",
            "Test acc:  29.291095579558863\n",
            "Test loss:  1.74357964331015\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.74357964331015\n",
            "Epoch 4: | Loss: 1.72706 | Acc: 0.306\n",
            "Test acc:  31.206317509303805\n",
            "Test loss:  1.728218803516013\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.728218803516013\n",
            "Epoch 5: | Loss: 1.70340 | Acc: 0.322\n",
            "Test acc:  32.477080875011346\n",
            "Test loss:  1.6983413799649718\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6983413799649718\n",
            "Epoch 6: | Loss: 1.66194 | Acc: 0.345\n",
            "Test acc:  34.62830171553055\n",
            "Test loss:  1.6598579380553582\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6598579380553582\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-00d70f174dbc>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtot_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}