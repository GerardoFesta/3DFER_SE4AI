{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardoFesta/3DFER_SE4AI/blob/1-poor-performances-on-test-set/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ind-xwGX3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbae33e2-0b7d-4d01-bfea-86d182d71a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Username: gfesta24@gmail.com\n",
            "Password: \n",
            "Repeat for confirmation: \n"
          ]
        }
      ],
      "source": [
        "!databricks configure --host https://community.cloud.databricks.com/\n",
        "!pip install -q mlflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import mlflow\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "TRAIN_PATH = \"train\"\n",
        "TEST_PATH =\"test\"\n"
      ],
      "metadata": {
        "id": "gsen0EXzN_aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "mlflow.set_experiment(\"/Users/gfesta24@gmail.com/2DFERResNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BTZqxSaSB94",
        "outputId": "e10c9b95-5f89-4d5c-bb39-2ec402eebdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/4292817106382685', creation_time=1685048826432, experiment_id='4292817106382685', last_update_time=1686041966112, lifecycle_stage='active', name='/Users/gfesta24@gmail.com/2DFERResNet', tags={'mlflow.experiment.sourceName': '/Users/gfesta24@gmail.com/2DFERResNet',\n",
              " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
              " 'mlflow.ownerEmail': 'gfesta24@gmail.com',\n",
              " 'mlflow.ownerId': '1923923806180228'}>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "tBv43qjlRIAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#Questo commento serve per provare il funzionamento dei commit con colab\n",
        "\n",
        "batch_size = 1024 "
      ],
      "metadata": {
        "id": "fE7pCxDmOYph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('fer2013.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "iT26qYfvO8sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "class_dir=os.listdir(TRAIN_PATH+'/')\n",
        "folder_names = class_dir\n",
        "label_dict = {folder_names[i]:i for i in range(len(folder_names))}\n",
        "num_classes=len(label_dict)\n",
        "train_image_filenames = []\n",
        "train_labels = []\n",
        "\n",
        "# iterate through each folder and collect filenames and labels\n",
        "for folder_name in folder_names:\n",
        "    folder_path = os.path.join(TRAIN_PATH, folder_name)\n",
        "    for filename in os.listdir(folder_path):\n",
        "        train_image_filenames.append(os.path.join(TRAIN_PATH+\"/\"+folder_name, filename))\n",
        "        train_labels.append(label_dict[folder_name])\n",
        "\n",
        "# create pandas dataframe\n",
        "train_df = pd.DataFrame({'filename': train_image_filenames, 'emotion': train_labels})\n",
        "\n",
        "print(len(train_df))\n",
        "\n",
        "\n",
        "test_image_filenames = []\n",
        "test_labels = []\n",
        "class_dir=os.listdir(TEST_PATH+'/')\n",
        "for folder_name in folder_names:\n",
        "    folder_path = os.path.join(TEST_PATH, folder_name)\n",
        "    for filename in os.listdir(folder_path):\n",
        "        test_image_filenames.append(os.path.join(TEST_PATH+\"/\"+folder_name, filename))\n",
        "        test_labels.append(label_dict[folder_name])\n",
        "\n",
        "test_df = pd.DataFrame({'filename': test_image_filenames, 'emotion': test_labels})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp6VXk73PUy8",
        "outputId": "e8093d55-6fe0-4780-ada8-cfecaf0657d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['img_as_matrix'] = train_df['filename'].apply(lambda path: cv2.imread(path))\n",
        "test_df['img_as_matrix'] = test_df['filename'].apply(lambda path: cv2.imread(path))\n"
      ],
      "metadata": {
        "id": "AGSZTlxmPbut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.data = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.data.iloc[index]['img_as_matrix'] \n",
        "        label = self.data.iloc[index]['emotion'] \n",
        "\n",
        "        # Esegui le trasformazioni se definite\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "geovWMyTPhBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(columns=[\"filename\"], inplace=True)\n",
        "test_df.drop(columns=[\"filename\"], inplace=True)\n",
        "train_df.at[0,\"img_as_matrix\"]\n",
        "full_df = pd.concat([train_df, test_df])\n",
        "\n",
        "full_dataset = CustomDataset(full_df, transform=transforms.ToTensor())\n",
        "full_daset_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "#train_dataset = CustomDataset(train_df, transform=transforms.ToTensor())\n",
        "#test_dataset = CustomDataset(test_df, transform=transforms.ToTensor())\n",
        " \n",
        "\n",
        "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(full_df['img_as_matrix'], full_df['emotion'], test_size=0.33, random_state=42, stratify=full_df['emotion'])\n",
        "\n",
        "print(type(X_train))\n",
        "def mean_std_calc(loader):\n",
        "  cnt = 0\n",
        "  fst_moment = torch.empty(3)\n",
        "  snd_moment = torch.empty(3)\n",
        "\n",
        "  for images, _ in loader:\n",
        "      b, c, h, w = images.shape\n",
        "      nb_pixels = b * h * w\n",
        "      sum_ = torch.sum(images, dim=[0, 2, 3])\n",
        "      sum_of_square = torch.sum(images ** 2,\n",
        "                                dim=[0, 2, 3])\n",
        "      fst_moment = (cnt * fst_moment + sum_) / (\n",
        "                    cnt + nb_pixels)\n",
        "      snd_moment = (cnt * snd_moment + sum_of_square) / (\n",
        "                          cnt + nb_pixels)\n",
        "      cnt += nb_pixels\n",
        "\n",
        "  mean, std = fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)\n",
        "  return mean, std\n",
        "\n",
        "#Normalizzazione train loader\n",
        "mean, std = mean_std_calc(full_daset_loader)\n",
        "\n",
        "transform_img_normal = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std= std),\n",
        "])\n",
        "\n",
        "X_train.name=\"img_as_matrix\"\n",
        "X_test.name=\"img_as_matrix\"\n",
        "y_test.name=\"emotion\"\n",
        "y_train.name=\"emotion\"\n",
        "new_train_df=pd.concat([X_train, y_train], axis=1)\n",
        "new_test_df=pd.concat([X_test, y_test], axis=1)\n",
        "print(new_train_df)\n",
        "\n",
        "train_dataset = CustomDataset(new_train_df, transform=transform_img_normal)\n",
        "\n",
        "test_dataset = CustomDataset(new_test_df, transform=transform_img_normal)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QLPPzSbRPi-A",
        "outputId": "11e34430-4a75-4ab7-8730-190eff1f5ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "                                           img_as_matrix  emotion\n",
            "625    [[[48, 48, 48], [52, 52, 52], [54, 54, 54], [5...        0\n",
            "1345   [[[38, 38, 38], [45, 45, 45], [48, 48, 48], [5...        0\n",
            "9439   [[[144, 144, 144], [137, 137, 137], [134, 134,...        1\n",
            "6114   [[[108, 108, 108], [24, 24, 24], [0, 0, 0], [1...        6\n",
            "12430  [[[3, 3, 3], [1, 1, 1], [2, 2, 2], [4, 4, 4], ...        2\n",
            "...                                                  ...      ...\n",
            "23339  [[[253, 253, 253], [248, 248, 248], [252, 252,...        5\n",
            "21635  [[[182, 182, 182], [172, 172, 172], [154, 154,...        5\n",
            "14447  [[[63, 63, 63], [64, 64, 64], [58, 58, 58], [5...        2\n",
            "14240  [[[181, 181, 181], [175, 175, 175], [161, 161,...        2\n",
            "17100  [[[175, 175, 175], [148, 148, 148], [101, 101,...        3\n",
            "\n",
            "[24044 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "  probabilities = torch.nn.functional.softmax(preds, dim=1)\n",
        "  _, predicted = torch.max(probabilities, dim=1)\n",
        "  n_correct = (predicted==labels).sum().float()\n",
        "\n",
        "  acc =n_correct / labels.shape[0]\n",
        "  acc= torch.round(acc*100)\n",
        "  return acc, n_correct;"
      ],
      "metadata": {
        "id": "4rPc52-TPyEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet 50 Pytorch Code"
      ],
      "metadata": {
        "id": "c7t0zHoDHt2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class block(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.expansion = 4\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            intermediate_channels,\n",
        "            intermediate_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, image_channels, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, layers[0], intermediate_channels=64, stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, layers[1], intermediate_channels=128, stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, layers[2], intermediate_channels=256, stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, layers[3], intermediate_channels=512, stride=2\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(512 * 4, 512)\n",
        "        self.bn2 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)        \n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
        "        identity_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
        "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
        "        # to the layer that's ahead\n",
        "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
        "            identity_downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    intermediate_channels * 4,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(intermediate_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers.append(\n",
        "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
        "        )\n",
        "\n",
        "        # The expansion size is always 4 for ResNet 50,101,152\n",
        "        self.in_channels = intermediate_channels * 4\n",
        "\n",
        "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
        "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
        "        # and also same amount of channels.\n",
        "        for i in range(num_residual_blocks - 1):\n",
        "            layers.append(block(self.in_channels, intermediate_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet50(img_channel=3, num_classes=7):\n",
        "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n"
      ],
      "metadata": {
        "id": "Lig4-9-OH8S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()"
      ],
      "metadata": {
        "id": "rgvXWGRCDEXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {'batch_size': [64, 128, 256, 512],'lr': [0.01, 0.001, 0.0001], \"momentum\":[0.7, 0.8, 0.9], \"decay\":[0.001, 0.01, 0.1]}\n",
        "expanded_grid = ParameterGrid(param_grid)\n",
        "client = mlflow.tracking.MlflowClient()\n",
        "experiment = client.get_experiment_by_name(\"/Users/gfesta24@gmail.com/2DFERResNet\")\n",
        "\n",
        "\n",
        "for i in range(len(expanded_grid)):\n",
        "  runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id], filter_string=\" and \".join([f\"params.{k} = '{v}'\" for k, v in expanded_grid[i].items()]))\n",
        "\n",
        "  if len(runs) == 0:\n",
        "\n",
        "\n",
        "    best_loss=100\n",
        "    best_model_train_acc=0\n",
        "    best_model_test_acc=0\n",
        "    best_model_test_loss=0\n",
        "    best_model_train_loss=0\n",
        "    min_delta=0\n",
        "    patience=3\n",
        "    train_loader = DataLoader(train_dataset, batch_size=expanded_grid[i]['batch_size'], shuffle=True)\n",
        "    \n",
        "    test_loader = DataLoader(test_dataset, batch_size=expanded_grid[i]['batch_size'], shuffle=False)\n",
        "    model = ResNet50().to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=expanded_grid[i]['lr'], momentum =expanded_grid[i]['momentum'])\n",
        "\n",
        "\n",
        "    mlflow.start_run()\n",
        "\n",
        "\n",
        "    n_total_steps = len(train_loader)\n",
        "    num_epochs = 100\n",
        "    acc_list_train=[]\n",
        "    acc_list_test=[]\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    mlflow.set_tag(\"model_name\", \"ResNet50\")\n",
        "    mlflow.log_param(\"lr\", expanded_grid[i]['lr'])\n",
        "    mlflow.log_param(\"momentum\", expanded_grid[i]['momentum'])\n",
        "    mlflow.log_param(\"batch_size\", expanded_grid[i]['batch_size'])\n",
        "    mlflow.log_param(\"decay\", expanded_grid[i]['decay'])\n",
        "    best_loss = 100\n",
        "    counter=0\n",
        "    stop=False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(counter)\n",
        "        if stop:\n",
        "          print(stop)\n",
        "          break\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        seen = 0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "          \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(images)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          \n",
        "          _, acc = accuracy(outputs, labels)\n",
        "          seen +=labels.shape[0]\n",
        "\n",
        "          optimizer.zero_grad()      \n",
        "          loss.backward()            \n",
        "          optimizer.step()  \n",
        "          running_loss += loss.item()    \n",
        "          running_acc += acc\n",
        "\n",
        "        print (f'Epoch [{epoch}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Acc: {running_acc/seen:.4f}')\n",
        "        acc_list_train.append(running_acc/len(train_loader))\n",
        "        mlflow.log_metric(\"train_loss\", running_loss / len(train_loader), step=epoch)\n",
        "        mlflow.log_metric(\"train_acc\", running_acc/seen, step=epoch)\n",
        "        \n",
        "\n",
        "        tot_corrette = 0\n",
        "        tot_eseguite = 0\n",
        "        running_test_loss = 0\n",
        "        val_loss = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "\n",
        "          for images, labels in test_loader:\n",
        "              images = images.to(device)\n",
        "              labels = labels.to(device)\n",
        "            \n",
        "              outputs = model(images)\n",
        "              test_loss = criterion(outputs, labels)\n",
        "              _, n_corrette=accuracy(outputs, labels)\n",
        "              \n",
        "              running_test_loss += test_loss.item() \n",
        "              tot_corrette+=n_corrette.item()\n",
        "              tot_eseguite+=labels.shape[0]\n",
        "\n",
        "          test_acc=100* (tot_corrette/tot_eseguite)\n",
        "          val_loss = running_test_loss / len(test_loader)\n",
        "          acc_list_test.append(test_acc)\n",
        "          print(\"Test acc: \", test_acc)\n",
        "          print(\"Test loss: \", val_loss)\n",
        "          \n",
        "          mlflow.log_metric(\"test_acc\", test_acc, step=epoch)\n",
        "          mlflow.log_metric(\"test_loss\", val_loss, step=epoch)\n",
        "          \n",
        "          \n",
        "        if val_loss < best_loss - min_delta:\n",
        "          print(\"MIGLIORATO\")\n",
        "          best_loss = val_loss\n",
        "          best_model_train_acc=running_acc/seen\n",
        "          best_model_test_acc=test_acc\n",
        "          best_model_test_loss=val_loss\n",
        "          best_model_train_loss=running_loss / len(train_loader)\n",
        "          counter = 0\n",
        "          # Salva i pesi del modello se la validation loss è migliorata\n",
        "          torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "          counter += 1\n",
        "        # Verifica se raggiunto il criterio di early stopping\n",
        "          if counter >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              stop=True\n",
        "        print(\"BEST TEST LOSS: \", best_loss)\n",
        "\n",
        "    mlflow.set_tag(\"Epochs_stopped\", epoch+1)\n",
        "    mlflow.log_artifact(\"best_model.pt\")\n",
        "    mlflow.log_metric(\"best_test_acc\", best_model_test_acc)\n",
        "    mlflow.log_metric(\"best_test_loss\", best_model_test_loss)\n",
        "    mlflow.log_metric(\"best_train_acc\", best_model_train_acc)\n",
        "    mlflow.log_metric(\"best_train_loss\", best_model_train_loss)\n",
        "    mlflow.end_run()\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    PATH = './cnn.pth'\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    torch.cuda.empty_cache()\n",
        "  else:\n",
        "    print(\"RUN: \", [f\"params.{k} = '{v}'\" for k, v in expanded_grid[i].items()], \" già completata\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqTk-ssnQAMM",
        "outputId": "994c4cb8-0d86-45ad-81b5-b840c082b257"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.0001'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.0001'\", \"params.decay = '0.01'\", \"params.batch_size = '64'\"]  già completata\n",
            "0\n",
            "Epoch [0/100], Loss: 1.9655, Acc: 0.1854\n",
            "Test acc:  21.286836105716457\n",
            "Test loss:  1.9230206519044855\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9230206519044855\n",
            "0\n",
            "Epoch [1/100], Loss: 1.8958, Acc: 0.2338\n",
            "Test acc:  24.259055982436884\n",
            "Test loss:  1.8800239255351405\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8800239255351405\n",
            "0\n",
            "Epoch [2/100], Loss: 1.8579, Acc: 0.2604\n",
            "Test acc:  27.34948914970869\n",
            "Test loss:  1.8475101237655969\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8475101237655969\n",
            "0\n",
            "Epoch [3/100], Loss: 1.8162, Acc: 0.2893\n",
            "Test acc:  28.649835345773877\n",
            "Test loss:  1.8197780257912093\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8197780257912093\n",
            "0\n",
            "Epoch [4/100], Loss: 1.7716, Acc: 0.3147\n",
            "Test acc:  31.419403867263362\n",
            "Test loss:  1.7699334839338898\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7699334839338898\n",
            "0\n",
            "Epoch [5/100], Loss: 1.7085, Acc: 0.3498\n",
            "Test acc:  34.619606518618596\n",
            "Test loss:  1.7124867618724864\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7124867618724864\n",
            "0\n",
            "Epoch [6/100], Loss: 1.6465, Acc: 0.3780\n",
            "Test acc:  36.6798952967998\n",
            "Test loss:  1.673963149388631\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.673963149388631\n",
            "0\n",
            "Epoch [7/100], Loss: 1.5805, Acc: 0.4065\n",
            "Test acc:  37.32162458836443\n",
            "Test loss:  1.649111637825607\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.649111637825607\n",
            "0\n",
            "Epoch [8/100], Loss: 1.5175, Acc: 0.4307\n",
            "Test acc:  39.052604914295365\n",
            "Test loss:  1.620815348240637\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.620815348240637\n",
            "0\n",
            "Epoch [9/100], Loss: 1.4498, Acc: 0.4599\n",
            "Test acc:  38.84150975259647\n",
            "Test loss:  1.6243056693384725\n",
            "BEST TEST LOSS:  1.620815348240637\n",
            "1\n",
            "Epoch [10/100], Loss: 1.3872, Acc: 0.4864\n",
            "Test acc:  39.52545807650088\n",
            "Test loss:  1.621991495291392\n",
            "BEST TEST LOSS:  1.620815348240637\n",
            "2\n",
            "Epoch [11/100], Loss: 1.3148, Acc: 0.5197\n",
            "Test acc:  40.32761969095668\n",
            "Test loss:  1.6170111529288753\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.6170111529288753\n",
            "0\n",
            "Epoch [12/100], Loss: 1.2328, Acc: 0.5506\n",
            "Test acc:  41.41687072532297\n",
            "Test loss:  1.639619295955986\n",
            "BEST TEST LOSS:  1.6170111529288753\n",
            "1\n",
            "Epoch [13/100], Loss: 1.1373, Acc: 0.5882\n",
            "Test acc:  41.636409693489824\n",
            "Test loss:  1.6643078295133447\n",
            "BEST TEST LOSS:  1.6170111529288753\n",
            "2\n",
            "Epoch [14/100], Loss: 1.0357, Acc: 0.6291\n",
            "Test acc:  41.24799459596386\n",
            "Test loss:  1.7256905628788857\n",
            "Early stopping at epoch 15\n",
            "BEST TEST LOSS:  1.6170111529288753\n",
            "3\n",
            "True\n",
            "Finished Training\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.01'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.01'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.01'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.0001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.0001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.0001'\", \"params.decay = '0.1'\", \"params.batch_size = '64'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.01'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.0001'\", \"params.decay = '0.001'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.01'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.7'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.8'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "RUN:  [\"params.momentum = '0.9'\", \"params.lr = '0.001'\", \"params.decay = '0.01'\", \"params.batch_size = '128'\"]  già completata\n",
            "0\n",
            "Epoch [0/100], Loss: 2.0332, Acc: 0.1505\n",
            "Test acc:  15.688592417461791\n",
            "Test loss:  2.014728085969084\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  2.014728085969084\n",
            "0\n",
            "Epoch [1/100], Loss: 1.9958, Acc: 0.1605\n",
            "Test acc:  16.178333192603226\n",
            "Test loss:  1.9899455488369029\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9899455488369029\n",
            "0\n",
            "Epoch [2/100], Loss: 1.9737, Acc: 0.1724\n",
            "Test acc:  17.647555518027527\n",
            "Test loss:  1.9649964852999615\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9649964852999615\n",
            "0\n",
            "Epoch [3/100], Loss: 1.9600, Acc: 0.1820\n",
            "Test acc:  18.213290551380563\n",
            "Test loss:  1.9521357154333463\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9521357154333463\n",
            "0\n",
            "Epoch [4/100], Loss: 1.9448, Acc: 0.1907\n",
            "Test acc:  18.821244617073376\n",
            "Test loss:  1.938745161538483\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.938745161538483\n",
            "0\n",
            "Epoch [5/100], Loss: 1.9327, Acc: 0.2001\n",
            "Test acc:  19.952714683779448\n",
            "Test loss:  1.9339001986288256\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9339001986288256\n",
            "0\n",
            "Epoch [6/100], Loss: 1.9212, Acc: 0.2122\n",
            "Test acc:  20.847758169382757\n",
            "Test loss:  1.9283694092945387\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9283694092945387\n",
            "0\n",
            "Epoch [7/100], Loss: 1.9111, Acc: 0.2202\n",
            "Test acc:  21.185510428100987\n",
            "Test loss:  1.920544498710222\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.920544498710222\n",
            "0\n",
            "Epoch [8/100], Loss: 1.9053, Acc: 0.2257\n",
            "Test acc:  22.5956261082496\n",
            "Test loss:  1.9108202649701027\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9108202649701027\n",
            "0\n",
            "Epoch [9/100], Loss: 1.8985, Acc: 0.2335\n",
            "Test acc:  22.224098623659547\n",
            "Test loss:  1.911579344862251\n",
            "BEST TEST LOSS:  1.9108202649701027\n",
            "1\n",
            "Epoch [10/100], Loss: 1.8914, Acc: 0.2391\n",
            "Test acc:  23.220467786878324\n",
            "Test loss:  1.896626914701154\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.896626914701154\n",
            "0\n",
            "Epoch [11/100], Loss: 1.8787, Acc: 0.2495\n",
            "Test acc:  23.499113400320866\n",
            "Test loss:  1.8943550420063797\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8943550420063797\n",
            "0\n",
            "Epoch [12/100], Loss: 1.8721, Acc: 0.2545\n",
            "Test acc:  24.52081398294351\n",
            "Test loss:  1.8864600837871592\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8864600837871592\n",
            "0\n",
            "Epoch [13/100], Loss: 1.8577, Acc: 0.2656\n",
            "Test acc:  25.24698133918771\n",
            "Test loss:  1.8779200148838822\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8779200148838822\n",
            "0\n",
            "Epoch [14/100], Loss: 1.8511, Acc: 0.2683\n",
            "Test acc:  25.0949928227645\n",
            "Test loss:  1.8782455639172626\n",
            "BEST TEST LOSS:  1.8779200148838822\n",
            "1\n",
            "Epoch [15/100], Loss: 1.8402, Acc: 0.2772\n",
            "Test acc:  25.525626952630248\n",
            "Test loss:  1.8753391888833815\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8753391888833815\n",
            "0\n",
            "Epoch [16/100], Loss: 1.8364, Acc: 0.2778\n",
            "Test acc:  25.838047791944607\n",
            "Test loss:  1.8658317635136266\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8658317635136266\n",
            "0\n",
            "Epoch [17/100], Loss: 1.8267, Acc: 0.2865\n",
            "Test acc:  26.513552309381065\n",
            "Test loss:  1.8588709638964744\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8588709638964744\n",
            "0\n",
            "Epoch [18/100], Loss: 1.8099, Acc: 0.2931\n",
            "Test acc:  27.096174955670016\n",
            "Test loss:  1.8551849485725485\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8551849485725485\n",
            "0\n",
            "Epoch [19/100], Loss: 1.7943, Acc: 0.3055\n",
            "Test acc:  27.08773114920206\n",
            "Test loss:  1.8496354292797785\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8496354292797785\n",
            "0\n",
            "Epoch [20/100], Loss: 1.7867, Acc: 0.3084\n",
            "Test acc:  27.830786118382168\n",
            "Test loss:  1.8434356656125797\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8434356656125797\n",
            "0\n",
            "Epoch [21/100], Loss: 1.7663, Acc: 0.3249\n",
            "Test acc:  28.337414506459513\n",
            "Test loss:  1.8342079436907204\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8342079436907204\n",
            "0\n",
            "Epoch [22/100], Loss: 1.7536, Acc: 0.3295\n",
            "Test acc:  28.71738579751752\n",
            "Test loss:  1.827633096325782\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.827633096325782\n",
            "0\n",
            "Epoch [23/100], Loss: 1.7361, Acc: 0.3376\n",
            "Test acc:  29.764417799544034\n",
            "Test loss:  1.812920799819372\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.812920799819372\n",
            "0\n",
            "Epoch [24/100], Loss: 1.7232, Acc: 0.3484\n",
            "Test acc:  29.848855864223594\n",
            "Test loss:  1.8133921648866387\n",
            "BEST TEST LOSS:  1.812920799819372\n",
            "1\n",
            "Epoch [25/100], Loss: 1.7094, Acc: 0.3545\n",
            "Test acc:  30.64257367221143\n",
            "Test loss:  1.8006789774023078\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8006789774023078\n",
            "0\n",
            "Epoch [26/100], Loss: 1.6931, Acc: 0.3590\n",
            "Test acc:  31.14920206028878\n",
            "Test loss:  1.7882940666649931\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7882940666649931\n",
            "0\n",
            "Epoch [27/100], Loss: 1.6656, Acc: 0.3722\n",
            "Test acc:  31.1829772861606\n",
            "Test loss:  1.786438686873323\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.786438686873323\n",
            "0\n",
            "Epoch [28/100], Loss: 1.6441, Acc: 0.3798\n",
            "Test acc:  32.47487967575783\n",
            "Test loss:  1.7715470290953113\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7715470290953113\n",
            "0\n",
            "Epoch [29/100], Loss: 1.6211, Acc: 0.3917\n",
            "Test acc:  32.694418643924685\n",
            "Test loss:  1.7699679020912416\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7699679020912416\n",
            "0\n",
            "Epoch [30/100], Loss: 1.5946, Acc: 0.4030\n",
            "Test acc:  33.26859748374567\n",
            "Test loss:  1.7642587513052008\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7642587513052008\n",
            "0\n",
            "Epoch [31/100], Loss: 1.5706, Acc: 0.4149\n",
            "Test acc:  33.538799290720256\n",
            "Test loss:  1.7608346221267537\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7608346221267537\n",
            "0\n",
            "Epoch [32/100], Loss: 1.5359, Acc: 0.4274\n",
            "Test acc:  34.32407329224014\n",
            "Test loss:  1.753091601915257\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.753091601915257\n",
            "0\n",
            "Epoch [33/100], Loss: 1.5051, Acc: 0.4461\n",
            "Test acc:  34.78848264797771\n",
            "Test loss:  1.7472607345991238\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.7472607345991238\n",
            "0\n",
            "Epoch [34/100], Loss: 1.4748, Acc: 0.4565\n",
            "Test acc:  34.85603309972136\n",
            "Test loss:  1.747685472170512\n",
            "BEST TEST LOSS:  1.7472607345991238\n",
            "1\n",
            "Epoch [35/100], Loss: 1.4454, Acc: 0.4678\n",
            "Test acc:  35.252892003715274\n",
            "Test loss:  1.744209199823359\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.744209199823359\n",
            "0\n",
            "Epoch [36/100], Loss: 1.4085, Acc: 0.4800\n",
            "Test acc:  35.62441948830533\n",
            "Test loss:  1.7496267839144635\n",
            "BEST TEST LOSS:  1.744209199823359\n",
            "1\n",
            "Epoch [37/100], Loss: 1.3687, Acc: 0.5012\n",
            "Test acc:  35.97061555349151\n",
            "Test loss:  1.7509000583361554\n",
            "BEST TEST LOSS:  1.744209199823359\n",
            "2\n",
            "Epoch [38/100], Loss: 1.3316, Acc: 0.5163\n",
            "Test acc:  35.83551465000422\n",
            "Test loss:  1.7545893256382277\n",
            "Early stopping at epoch 39\n",
            "BEST TEST LOSS:  1.744209199823359\n",
            "3\n",
            "True\n",
            "Finished Training\n",
            "0\n",
            "Epoch [0/100], Loss: 2.0070, Acc: 0.1622\n",
            "Test acc:  16.532973064257366\n",
            "Test loss:  1.983684778213501\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.983684778213501\n",
            "0\n",
            "Epoch [1/100], Loss: 1.9663, Acc: 0.1778\n",
            "Test acc:  18.078189647893268\n",
            "Test loss:  1.953747498091831\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.953747498091831\n",
            "0\n",
            "Epoch [2/100], Loss: 1.9406, Acc: 0.1985\n",
            "Test acc:  20.096259393734694\n",
            "Test loss:  1.9345721403757732\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9345721403757732\n",
            "0\n",
            "Epoch [3/100], Loss: 1.9247, Acc: 0.2093\n",
            "Test acc:  21.430380815671704\n",
            "Test loss:  1.9205363386420793\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9205363386420793\n",
            "0\n",
            "Epoch [4/100], Loss: 1.9147, Acc: 0.2203\n",
            "Test acc:  22.004559655492695\n",
            "Test loss:  1.9155800957833566\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9155800957833566\n",
            "0\n",
            "Epoch [5/100], Loss: 1.8995, Acc: 0.2322\n",
            "Test acc:  23.110698302794898\n",
            "Test loss:  1.9025252839570403\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.9025252839570403\n",
            "0\n",
            "Epoch [6/100], Loss: 1.8841, Acc: 0.2432\n",
            "Test acc:  24.02262940133412\n",
            "Test loss:  1.889335379805616\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.889335379805616\n",
            "0\n",
            "Epoch [7/100], Loss: 1.8757, Acc: 0.2501\n",
            "Test acc:  24.529257789411467\n",
            "Test loss:  1.8803941857430242\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8803941857430242\n",
            "0\n",
            "Epoch [8/100], Loss: 1.8653, Acc: 0.2572\n",
            "Test acc:  25.103436629232455\n",
            "Test loss:  1.8701742797769525\n",
            "MIGLIORATO\n",
            "BEST TEST LOSS:  1.8701742797769525\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creazione dell'asse x con gli indici delle liste\n",
        "epochs = range(1, len(acc_list_train) + 1)\n",
        "\n",
        "# Tracciamento delle due variabili come linee di colori diversi\n",
        "plt.plot(epochs, acc_list_train, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, acc_list_test, 'r', label='Test Accuracy')\n",
        "\n",
        "# Titoli degli assi\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Aggiunta di una legenda\n",
        "plt.legend()\n",
        "\n",
        "# Visualizzazione del grafico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3JTADuzcEzcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    tot_corrette = 0\n",
        "    tot_eseguite = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "       \n",
        "        outputs = model(images)\n",
        "        _, n_corrette=accuracy(outputs, labels)\n",
        "        tot_corrette+=n_corrette\n",
        "        tot_eseguite+=labels.shape[0]"
      ],
      "metadata": {
        "id": "oJoj04GlT1vO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}